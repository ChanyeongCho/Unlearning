{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daef1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset, DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference\n",
    "from models import (CNNCifar, CNNMnist, ResNet18, \n",
    "                   GeneratorCifar, DiscriminatorCifar, Generator, Discriminator,\n",
    "                   generate_images, filter_images, add_backdoor_trigger_cifar, add_backdoor_trigger_mnist)\n",
    "from utils import get_dataset, average_weights, exp_details, create_poisoned_dataset\n",
    "from unlearn import (\n",
    "    train_generator_ungan,\n",
    "    SyntheticImageDataset,\n",
    "    partition_synthetic_data_iid,\n",
    "    get_synthetic_subset\n",
    ")\n",
    "from evaluate_mia import evaluate_mia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2fb9d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset, DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference\n",
    "from models import (CNNCifar, CNNMnist, ResNet18, \n",
    "                   GeneratorCifar, DiscriminatorCifar, Generator, Discriminator,\n",
    "                   generate_images, filter_images, add_backdoor_trigger_cifar, add_backdoor_trigger_mnist)\n",
    "from utils import get_dataset, average_weights, exp_details, create_poisoned_dataset\n",
    "from unlearn import (\n",
    "    train_generator_ungan,\n",
    "    SyntheticImageDataset,\n",
    "    partition_synthetic_data_iid,\n",
    "    get_synthetic_subset\n",
    ")\n",
    "from evaluate_mia import evaluate_mia\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "# =================== Image Visualization Functions ===================\n",
    "\n",
    "def denormalize_image(tensor, dataset_type='cifar'):\n",
    "    \"\"\"데이터셋 타입에 따른 정규화 해제\"\"\"\n",
    "    if dataset_type == 'cifar':\n",
    "        mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "    else:  # mnist\n",
    "        mean = torch.tensor([0.1307]).view(1, 1, 1)\n",
    "        std = torch.tensor([0.3081]).view(1, 1, 1)\n",
    "    \n",
    "    denormalized = tensor * std + mean\n",
    "    return torch.clamp(denormalized, 0, 1)\n",
    "\n",
    "\n",
    "def visualize_hybrid_generation(generator, discriminator, dataset, unseen_dataset, \n",
    "                               forget_idxs, device, z_dim=100, num_samples=16, dataset_type='cifar'):\n",
    "    \"\"\"Hybrid 이미지 생성 과정을 단계별로 시각화\"\"\"\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    # 1. 원본 Forget 이미지들\n",
    "    forget_samples = []\n",
    "    forget_labels = []\n",
    "    for i in range(min(num_samples//4, len(forget_idxs))):\n",
    "        img, label = dataset[forget_idxs[i]]\n",
    "        forget_samples.append(img)\n",
    "        forget_labels.append(label)\n",
    "    \n",
    "    # 2. Unseen 이미지들\n",
    "    unseen_samples = []\n",
    "    for i in range(min(num_samples//4, len(unseen_dataset))):\n",
    "        img, _ = unseen_dataset[i]\n",
    "        unseen_samples.append(img)\n",
    "    \n",
    "    # 3. 기본 생성 이미지\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn((num_samples//4, z_dim), device=device)\n",
    "        basic_generated = generator(noise).cpu()\n",
    "    \n",
    "    # 4. Hybrid 이미지\n",
    "    unseen_style = torch.stack(unseen_samples).mean(dim=0, keepdim=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn((num_samples//4, z_dim), device=device)\n",
    "        gen_imgs = generator(noise)\n",
    "        hybrid_generated = style_blend(gen_imgs, unseen_style, strength=0.3).cpu()\n",
    "    \n",
    "    # 시각화\n",
    "    fig, axes = plt.subplots(4, num_samples//4, figsize=(16, 12))\n",
    "    fig.suptitle(f'Advanced UNGAN: Hybrid Image Generation Process ({dataset_type.upper()})', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    stages = [\n",
    "        (forget_samples, \"1. Original Forget Data\", 'Reds'),\n",
    "        (unseen_samples, \"2. Unseen Style Reference\", 'Blues'), \n",
    "        (basic_generated, \"3. Basic Generated\", 'Greens'),\n",
    "        (hybrid_generated, \"4. Hybrid (Forget+Unseen)\", 'Purples')\n",
    "    ]\n",
    "    \n",
    "    for row, (images, title, cmap) in enumerate(stages):\n",
    "        for col in range(len(images)):\n",
    "            ax = axes[row, col]\n",
    "            img = denormalize_image(images[col], dataset_type)\n",
    "            \n",
    "            # MNIST는 흑백, CIFAR는 컬러\n",
    "            if dataset_type == 'mnist':\n",
    "                img_np = img.squeeze().numpy()  # (28, 28)\n",
    "                ax.imshow(img_np, cmap='gray')\n",
    "            else:\n",
    "                img_np = img.permute(1, 2, 0).numpy()  # (32, 32, 3)\n",
    "                ax.imshow(img_np)\n",
    "            \n",
    "            ax.axis('off')\n",
    "            \n",
    "            if col == 0:\n",
    "                ax.text(-0.1, 0.5, title, rotation=90, verticalalignment='center',\n",
    "                       fontsize=12, fontweight='bold', transform=ax.transAxes)\n",
    "            \n",
    "            if row == 0:\n",
    "                ax.set_title(f'Class: {forget_labels[col]}', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./hybrid_generation_process_{dataset_type}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return hybrid_generated\n",
    "\n",
    "\n",
    "def save_individual_images(hybrid_images, save_dir='./generated_samples', dataset_type='cifar'):\n",
    "    \"\"\"개별 hybrid 이미지들을 파일로 저장\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    for i, img in enumerate(hybrid_images):\n",
    "        img_denorm = denormalize_image(img, dataset_type)\n",
    "        \n",
    "        if dataset_type == 'mnist':\n",
    "            # MNIST: 흑백 이미지 처리\n",
    "            img_denorm = img_denorm.squeeze()  # (1, 28, 28) -> (28, 28)\n",
    "        \n",
    "        img_pil = transforms.ToPILImage()(img_denorm)\n",
    "        img_pil.save(os.path.join(save_dir, f'hybrid_sample_{dataset_type}_{i:03d}.png'))\n",
    "    \n",
    "    print(f\" Saved {len(hybrid_images)} {dataset_type.upper()} hybrid images to {save_dir}/\")\n",
    "\n",
    "\n",
    "def visualize_advanced_ungan_results(generator, discriminator, dataset, unseen_dataset,\n",
    "                                   forget_idxs, filtered_images, filtered_labels, \n",
    "                                   device, args, dataset_type='cifar'):\n",
    "    \"\"\"Advanced UNGAN 결과를 종합적으로 시각화\"\"\"\n",
    "    print(f\"\\n ========== Visualizing Advanced UNGAN Results ({dataset_type.upper()}) ==========\")\n",
    "    \n",
    "    # 1. Hybrid 생성 과정 시각화\n",
    "    print(\" 1. Creating hybrid generation process visualization...\")\n",
    "    try:\n",
    "        hybrid_samples = visualize_hybrid_generation(\n",
    "            generator, discriminator, dataset, unseen_dataset, \n",
    "            forget_idxs, device, args.z_dim, num_samples=16, dataset_type=dataset_type\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"  Visualization error: {e}\")\n",
    "        hybrid_samples = filtered_images[:16] if len(filtered_images) >= 16 else filtered_images\n",
    "    \n",
    "    # 2. 개별 이미지 저장\n",
    "    print(\" 2. Saving individual hybrid images...\")\n",
    "    if len(filtered_images) > 0:\n",
    "        save_individual_images(filtered_images[:50], dataset_type=dataset_type)\n",
    "        \n",
    "        # 비교용 원본 이미지도 저장\n",
    "        os.makedirs('./original_samples', exist_ok=True)\n",
    "        for i, idx in enumerate(forget_idxs[:20]):\n",
    "            img, label = dataset[idx]\n",
    "            img_denorm = denormalize_image(img, dataset_type)\n",
    "            \n",
    "            if dataset_type == 'mnist':\n",
    "                img_denorm = img_denorm.squeeze()\n",
    "            \n",
    "            img_pil = transforms.ToPILImage()(img_denorm)\n",
    "            img_pil.save(f'./original_samples/forget_sample_{dataset_type}_{i:03d}_class{label}.png')\n",
    "        \n",
    "        print(f\" Also saved original {dataset_type.upper()} forget samples to ./original_samples/\")\n",
    "    else:\n",
    "        print(\"  No hybrid images to save\")\n",
    "    \n",
    "    print(\" Visualization completed! Check these files:\")\n",
    "    print(f\"    hybrid_generation_process_{dataset_type}.png - 4단계 생성 과정\")\n",
    "    print(\"    ./generated_samples/ - Hybrid 이미지들\")  \n",
    "    print(\"   ./original_samples/ - 원본 Forget 이미지들\")\n",
    "    \n",
    "    return hybrid_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495d7781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== Advanced UNGAN Functions ===================\n",
    "\n",
    "def extract_intermediate_features(discriminator, images):\n",
    "    \"\"\"Discriminator의 중간 레이어에서 특징 벡터 추출\"\"\"\n",
    "    features = images\n",
    "    for i, layer in enumerate(discriminator.model):\n",
    "        features = layer(features)\n",
    "        if i == 4:  # 중간 레이어에서 특징 추출\n",
    "            break\n",
    "    return features.view(features.size(0), -1)\n",
    "\n",
    "\n",
    "def style_blend(generated_imgs, style_reference, strength=0.3):\n",
    "    \"\"\"생성된 이미지에 참조 스타일 블렌딩\"\"\"\n",
    "    # 색조 및 밝기 조정\n",
    "    gen_mean = generated_imgs.mean(dim=[2, 3], keepdim=True)\n",
    "    gen_std = generated_imgs.std(dim=[2, 3], keepdim=True)\n",
    "    \n",
    "    style_mean = style_reference.mean(dim=[2, 3], keepdim=True)\n",
    "    style_std = style_reference.std(dim=[2, 3], keepdim=True)\n",
    "    \n",
    "    # 스타일 전이 (AdaIN 방식 간소화)\n",
    "    normalized = (generated_imgs - gen_mean) / (gen_std + 1e-8)\n",
    "    stylized = normalized * (gen_std * (1-strength) + style_std * strength) + \\\n",
    "               (gen_mean * (1-strength) + style_mean * strength)\n",
    "    \n",
    "    return torch.clamp(stylized, -1, 1)\n",
    "\n",
    "\n",
    "def train_hybrid_generator(generator, discriminator, dataset, unseen_dataset, \n",
    "                          retain_idxs, forget_idxs, device,\n",
    "                          alpha=0.7, beta=0.3, z_dim=100, batch_size=64, epochs=50):\n",
    "    \"\"\"\n",
    "    Forget 특성 + Unseen 특성을 융합하는 고급 Generator 훈련\n",
    "    \"\"\"\n",
    "    print(f\"[Hybrid UNGAN] Training with α={alpha} (forget) + β={beta} (unseen)\")\n",
    "    \n",
    "    g_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)\n",
    "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "    \n",
    "    # 데이터 로더 준비\n",
    "    forget_subset = Subset(dataset, forget_idxs)\n",
    "    forget_loader = DataLoader(forget_subset, batch_size=batch_size//2, shuffle=True, drop_last=True)\n",
    "    unseen_loader = DataLoader(unseen_dataset, batch_size=batch_size//2, shuffle=True, drop_last=True)\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_g_loss = 0\n",
    "        epoch_d_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # 두 데이터 로더를 동시에 순회\n",
    "        forget_iter = iter(forget_loader)\n",
    "        unseen_iter = iter(unseen_loader)\n",
    "        \n",
    "        for _ in range(min(len(forget_loader), len(unseen_loader))):\n",
    "            try:\n",
    "                forget_batch, forget_labels = next(forget_iter)\n",
    "                unseen_batch, unseen_labels = next(unseen_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "                \n",
    "            forget_batch = forget_batch.to(device)\n",
    "            unseen_batch = unseen_batch.to(device)\n",
    "            \n",
    "            batch_size_actual = min(forget_batch.size(0), unseen_batch.size(0))\n",
    "            forget_batch = forget_batch[:batch_size_actual]\n",
    "            unseen_batch = unseen_batch[:batch_size_actual]\n",
    "            \n",
    "            # Discriminator 훈련\n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            # 실제 데이터 (forget=0, unseen=1)\n",
    "            real_forget_labels = torch.zeros((batch_size_actual, 1), device=device)\n",
    "            real_unseen_labels = torch.ones((batch_size_actual, 1), device=device)\n",
    "            fake_labels = torch.full((batch_size_actual, 1), 0.5, device=device)\n",
    "            \n",
    "            # Discriminator 손실 계산\n",
    "            d_pred_forget = discriminator(forget_batch)\n",
    "            d_loss_forget = F.binary_cross_entropy(d_pred_forget, real_forget_labels)\n",
    "            \n",
    "            d_pred_unseen = discriminator(unseen_batch)\n",
    "            d_loss_unseen = F.binary_cross_entropy(d_pred_unseen, real_unseen_labels)\n",
    "            \n",
    "            z = torch.randn((batch_size_actual, z_dim), device=device)\n",
    "            fake_imgs = generator(z)\n",
    "            d_pred_fake = discriminator(fake_imgs.detach())\n",
    "            d_loss_fake = F.binary_cross_entropy(d_pred_fake, fake_labels)\n",
    "            \n",
    "            d_loss = d_loss_forget + d_loss_unseen + d_loss_fake\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            # Generator 훈련\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            z = torch.randn((batch_size_actual, z_dim), device=device)\n",
    "            gen_imgs = generator(z)\n",
    "            \n",
    "            # Strategy A: Forget 특성 유지\n",
    "            d_pred_gen = discriminator(gen_imgs)\n",
    "            forget_target = torch.zeros((batch_size_actual, 1), device=device)\n",
    "            loss_forget_mimic = F.binary_cross_entropy(d_pred_gen, forget_target)\n",
    "            \n",
    "            # Strategy B: Unseen 특성 모방\n",
    "            try:\n",
    "                unseen_features = extract_intermediate_features(discriminator, unseen_batch)\n",
    "                gen_features = extract_intermediate_features(discriminator, gen_imgs)\n",
    "                loss_unseen_style = F.mse_loss(gen_features, unseen_features)\n",
    "            except:\n",
    "                # 특징 추출 실패 시 픽셀 레벨 유사성 사용\n",
    "                loss_unseen_style = F.mse_loss(gen_imgs.mean(dim=[2,3]), unseen_batch.mean(dim=[2,3]))\n",
    "            \n",
    "            # 총 Generator 손실\n",
    "            g_loss = alpha * loss_forget_mimic + beta * loss_unseen_style\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            epoch_g_loss += g_loss.item()\n",
    "            epoch_d_loss += d_loss.item()\n",
    "            batch_count += 1\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            avg_g_loss = epoch_g_loss / max(batch_count, 1)\n",
    "            avg_d_loss = epoch_d_loss / max(batch_count, 1)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | G Loss: {avg_g_loss:.4f} | D Loss: {avg_d_loss:.4f}\")\n",
    "    \n",
    "    print(\"[Hybrid UNGAN] Training completed!\")\n",
    "    return generator\n",
    "\n",
    "\n",
    "def generate_hybrid_images(generator, forget_idxs, unseen_dataset, dataset, \n",
    "                          device='cpu', z_dim=100, style_strength=0.3):\n",
    "    \"\"\"Forget 내용 + Unseen 스타일의 hybrid 이미지 생성\"\"\"\n",
    "    generator.eval()\n",
    "    device = torch.device(device)\n",
    "    num_samples = len(forget_idxs)\n",
    "    \n",
    "    # Forget 데이터의 레이블 정보 유지\n",
    "    forget_labels = torch.tensor([dataset[i][1] for i in forget_idxs], dtype=torch.long)\n",
    "    \n",
    "    # Unseen 데이터에서 스타일 참조\n",
    "    unseen_samples = torch.stack([unseen_dataset[i][0] for i in range(min(100, len(unseen_dataset)))])\n",
    "    unseen_style = unseen_samples.mean(dim=0, keepdim=True).to(device)\n",
    "    \n",
    "    generated_images = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_size = 32\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_end = min(i + batch_size, num_samples)\n",
    "            batch_size_actual = batch_end - i\n",
    "            \n",
    "            noise = torch.randn((batch_size_actual, z_dim), device=device)\n",
    "            gen_imgs = generator(noise)\n",
    "            \n",
    "            # Post-processing: Unseen 스타일 블렌딩\n",
    "            if style_strength > 0:\n",
    "                gen_imgs = style_blend(gen_imgs, unseen_style, strength=style_strength)\n",
    "            \n",
    "            generated_images.append(gen_imgs.cpu())\n",
    "    \n",
    "    final_images = torch.cat(generated_images, dim=0)\n",
    "    return final_images, forget_labels\n",
    "\n",
    "\n",
    "def advanced_filter_images(discriminator, images, labels, \n",
    "                          forget_threshold=0.6, device='cpu'):\n",
    "    \"\"\"고급 필터링: Forget처럼 보이면서도 품질이 좋은 이미지만 선별\"\"\"\n",
    "    discriminator.eval()\n",
    "    device = torch.device(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images_gpu = images.to(device)\n",
    "        d_preds = discriminator(images_gpu).squeeze()\n",
    "        \n",
    "        # Forget 특성 유지 (낮은 값 = forget으로 분류)\n",
    "        forget_mask = d_preds < forget_threshold\n",
    "        \n",
    "        # 품질 필터링\n",
    "        quality_mask = (d_preds > 0.1) & (d_preds < 0.9)\n",
    "        \n",
    "        # 최종 마스크\n",
    "        final_mask = forget_mask & quality_mask\n",
    "        \n",
    "        filtered_imgs = images[final_mask]\n",
    "        filtered_labels = labels[final_mask]\n",
    "        \n",
    "        print(f\"[Advanced Filter] {len(images)} → {len(filtered_imgs)} images\")\n",
    "        print(f\"  Forget-like: {forget_mask.sum().item()}\")\n",
    "        print(f\"  Quality: {quality_mask.sum().item()}\")\n",
    "        print(f\"  Final: {final_mask.sum().item()}\")\n",
    "    \n",
    "    return filtered_imgs, filtered_labels\n",
    "\n",
    "\n",
    "def advanced_ungan_workflow(generator, discriminator, dataset, unseen_dataset,\n",
    "                           retain_idxs, forget_idxs, args, device):\n",
    "    \"\"\"Forget 특성 + Unseen 보안성을 결합한 고급 UNGAN 워크플로우\"\"\"\n",
    "    print(\"\\n========== Advanced UNGAN: Forget + Unseen Fusion ==========\")\n",
    "    \n",
    "    # Step 1: Hybrid Generator 훈련\n",
    "    print(\"Step 1: Training Hybrid Generator...\")\n",
    "    trained_generator = train_hybrid_generator(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator, \n",
    "        dataset=dataset,\n",
    "        unseen_dataset=unseen_dataset,\n",
    "        retain_idxs=retain_idxs,\n",
    "        forget_idxs=forget_idxs,\n",
    "        device=device,\n",
    "        alpha=0.7,  # Forget 특성 70%\n",
    "        beta=0.3,   # Unseen 특성 30%\n",
    "        z_dim=args.z_dim,\n",
    "        batch_size=args.local_bs,\n",
    "        epochs=30\n",
    "    )\n",
    "    \n",
    "    # Step 2: Hybrid 이미지 생성\n",
    "    print(\"Step 2: Generating Hybrid Images...\")\n",
    "    hybrid_images, hybrid_labels = generate_hybrid_images(\n",
    "        generator=trained_generator,\n",
    "        forget_idxs=forget_idxs,\n",
    "        unseen_dataset=unseen_dataset,\n",
    "        dataset=dataset,\n",
    "        device=device,\n",
    "        z_dim=args.z_dim,\n",
    "        style_strength=0.3\n",
    "    )\n",
    "    \n",
    "    # Step 3: 고급 필터링\n",
    "    print(\"Step 3: Advanced Filtering...\")\n",
    "    filtered_images, filtered_labels = advanced_filter_images(\n",
    "        discriminator=discriminator,\n",
    "        images=hybrid_images,\n",
    "        labels=hybrid_labels,\n",
    "        forget_threshold=0.6,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated {len(hybrid_images)} hybrid samples\")\n",
    "    print(f\"Filtered to {len(filtered_images)} high-quality hybrid samples\")\n",
    "    if len(hybrid_images) > 0:\n",
    "        print(f\"Quality ratio: {len(filtered_images)/len(hybrid_images)*100:.1f}%\")\n",
    "    \n",
    "    return filtered_images, filtered_labels, trained_generator\n",
    "\n",
    "\n",
    "def evaluate_hybrid_quality(model, filtered_images, filtered_labels, \n",
    "                           forget_idxs, unseen_dataset, dataset, device):\n",
    "    \"\"\"Hybrid 데이터의 품질 평가\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if len(filtered_images) == 0:\n",
    "        print(\"No filtered images to evaluate\")\n",
    "        return {}\n",
    "    \n",
    "    # 1. Forget 특성 유지도 평가\n",
    "    forget_sample_size = min(50, len(forget_idxs))\n",
    "    forget_imgs = torch.stack([dataset[i][0] for i in forget_idxs[:forget_sample_size]])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Original forget 데이터 예측\n",
    "        forget_logits = model(forget_imgs.to(device))\n",
    "        forget_preds = F.softmax(forget_logits, dim=1)\n",
    "        \n",
    "        # Hybrid 데이터 예측\n",
    "        hybrid_sample_size = min(50, len(filtered_images))\n",
    "        hybrid_logits = model(filtered_images[:hybrid_sample_size].to(device))\n",
    "        hybrid_preds = F.softmax(hybrid_logits, dim=1)\n",
    "        \n",
    "        # 클래스 분포 유사성\n",
    "        class_similarity = F.cosine_similarity(\n",
    "            forget_preds.mean(dim=0), \n",
    "            hybrid_preds.mean(dim=0), \n",
    "            dim=0\n",
    "        ).item()\n",
    "    \n",
    "    # 2. 스타일 통계 비교\n",
    "    unseen_sample_size = min(50, len(unseen_dataset))\n",
    "    unseen_sample = torch.stack([unseen_dataset[i][0] for i in range(unseen_sample_size)])\n",
    "    \n",
    "    forget_stats = (forget_imgs.mean().item(), forget_imgs.std().item())\n",
    "    hybrid_stats = (filtered_images[:hybrid_sample_size].mean().item(), \n",
    "                   filtered_images[:hybrid_sample_size].std().item())\n",
    "    unseen_stats = (unseen_sample.mean().item(), unseen_sample.std().item())\n",
    "    \n",
    "    # Unseen에 대한 유사도\n",
    "    style_similarity = 1 - abs(hybrid_stats[0] - unseen_stats[0]) - abs(hybrid_stats[1] - unseen_stats[1])\n",
    "    \n",
    "    results = {\n",
    "        'content_preservation': class_similarity,\n",
    "        'style_similarity': style_similarity,\n",
    "        'forget_stats': forget_stats,\n",
    "        'hybrid_stats': hybrid_stats,\n",
    "        'unseen_stats': unseen_stats\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n--- Hybrid Quality Evaluation ---\")\n",
    "    print(f\"Content Preservation: {class_similarity:.4f}\")\n",
    "    print(f\"Style Similarity: {style_similarity:.4f}\")\n",
    "    print(f\"Forget Stats: μ={forget_stats[0]:.3f}, σ={forget_stats[1]:.3f}\")\n",
    "    print(f\"Hybrid Stats: μ={hybrid_stats[0]:.3f}, σ={hybrid_stats[1]:.3f}\")\n",
    "    print(f\"Unseen Stats: μ={unseen_stats[0]:.3f}, σ={unseen_stats[1]:.3f}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4832c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== Main Functions ===================\n",
    "\n",
    "def move_dataset_to_device(dataset, device):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for x, y in dataset:\n",
    "        images.append(x.to(device))\n",
    "        labels.append(torch.tensor(y).to(device))\n",
    "    return TensorDataset(torch.stack(images), torch.stack(labels))\n",
    "\n",
    "\n",
    "def evaluate_backdoor_asr(model, dataset, target_label, device, dataset_type='cifar'):\n",
    "    \"\"\"데이터셋 타입에 따른 백도어 ASR 평가\"\"\"\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # 데이터셋 타입에 따른 백도어 함수 선택\n",
    "    if dataset_type == 'cifar':\n",
    "        backdoor_func = add_backdoor_trigger_cifar\n",
    "    else:  # mnist\n",
    "        backdoor_func = add_backdoor_trigger_mnist\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            x, y = dataset[i]\n",
    "            x_bd = backdoor_func(x).to(device)\n",
    "            x_bd = x_bd.unsqueeze(0)\n",
    "\n",
    "            output = model(x_bd)\n",
    "            pred = output.argmax(dim=1).item()\n",
    "\n",
    "            total += 1\n",
    "            if pred == target_label:\n",
    "                correct += 1\n",
    "\n",
    "    asr = correct / total\n",
    "    return asr\n",
    "\n",
    "\n",
    "def select_model(args):\n",
    "    \"\"\"데이터셋과 모델 타입에 따른 모델 선택\"\"\"\n",
    "    if args.dataset == 'cifar':\n",
    "        if args.model == 'cnn':\n",
    "            return CNNCifar(args=args)\n",
    "        elif args.model in ['resnet', 'resnet18']:\n",
    "            return ResNet18(num_classes=args.num_classes)\n",
    "    elif args.dataset == 'mnist':\n",
    "        if args.model == 'cnn':\n",
    "            return CNNMnist(args=args)\n",
    "        elif args.model in ['resnet', 'resnet18']:\n",
    "            # MNIST용 ResNet18 (입력 채널 1개로 수정)\n",
    "            model = ResNet18(num_classes=args.num_classes)\n",
    "            # 첫 번째 conv 레이어를 1채널로 변경\n",
    "            model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            return model\n",
    "    \n",
    "    raise NotImplementedError(f\"Model {args.model} not implemented for {args.dataset}\")\n",
    "\n",
    "\n",
    "def select_generator_discriminator(args):\n",
    "    \"\"\"데이터셋에 맞는 Generator/Discriminator 선택\"\"\"\n",
    "    if args.dataset == 'cifar':\n",
    "        img_shape = (3, 32, 32)\n",
    "        generator = GeneratorCifar(z_dim=args.z_dim, img_shape=img_shape)\n",
    "        discriminator = DiscriminatorCifar(img_shape=img_shape)\n",
    "    elif args.dataset == 'mnist':\n",
    "        img_shape = (1, 28, 28)\n",
    "        generator = Generator(z_dim=args.z_dim, img_shape=img_shape)\n",
    "        discriminator = Discriminator(img_shape=img_shape)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Dataset {args.dataset} not supported\")\n",
    "    \n",
    "    return generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f29af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    args = args_parser()\n",
    "    device = 'cuda' if args.gpu and torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    exp_details(args)\n",
    "\n",
    "    # ===================== 1. 데이터셋 로딩 및 초기화 =====================\n",
    "    train_dataset, test_dataset, unseen_dataset, user_groups = get_dataset(args)\n",
    "    \n",
    "    print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "    print(f\"Test dataset: {len(test_dataset)} samples\") \n",
    "    print(f\"Unseen dataset: {len(unseen_dataset)} samples\")\n",
    "    \n",
    "    # 백도어 독성 데이터셋 생성\n",
    "    full_dataset, user_groups = create_poisoned_dataset(train_dataset, user_groups, args,\n",
    "                                                        malicious_client=0,\n",
    "                                                        target_label=6,\n",
    "                                                        poison_ratio=0.8)\n",
    "\n",
    "    # 모델 초기화 (데이터셋별 자동 선택)\n",
    "    global_model = select_model(args).to(device)\n",
    "    global_model.train()\n",
    "\n",
    "    # Generator/Discriminator 초기화 (데이터셋별 자동 선택)\n",
    "    generator, discriminator = select_generator_discriminator(args)\n",
    "    generator = generator.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "\n",
    "    global_weights = global_model.state_dict()\n",
    "    train_loss, train_accuracy = [], []\n",
    "\n",
    "    # Unlearning 대상 설정\n",
    "    forget_client = 0\n",
    "    forget_idxs = user_groups[forget_client]\n",
    "    retain_idxs = [i for i in range(len(train_dataset)) if i not in forget_idxs]\n",
    "    test_idxs = np.random.choice(len(test_dataset), min(len(forget_idxs), len(test_dataset)), replace=False)\n",
    "\n",
    "    print(f\"\\nModel: {args.model.upper()}\")\n",
    "    print(f\"Forget client: {forget_client}\")\n",
    "    print(f\"Forget data size: {len(forget_idxs)}\")\n",
    "    print(f\"Retain data size: {len(retain_idxs)}\")\n",
    "\n",
    "    # ===================== 2. 연합 학습 (FedAvg) =====================\n",
    "    print(\"\\n========== Starting Federated Learning ==========\")\n",
    "    for epoch in tqdm(range(args.epochs), desc='Global Training Rounds'):\n",
    "        local_weights, local_losses = [], []\n",
    "        m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "        for idx in idxs_users:\n",
    "            if idx == forget_client:\n",
    "                continue  # 악성 클라이언트 제외\n",
    "\n",
    "            local_model = LocalUpdate(args=args, dataset=full_dataset, idxs=user_groups[idx])\n",
    "            w, loss = local_model.update_weights(model=copy.deepcopy(global_model), global_round=epoch)\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(loss)\n",
    "\n",
    "        if local_weights:\n",
    "            global_weights = average_weights(local_weights)\n",
    "            global_model.load_state_dict(global_weights)\n",
    "\n",
    "        loss_avg = sum(local_losses) / len(local_losses) if local_losses else 0\n",
    "        acc, _ = test_inference(args, global_model, test_dataset)\n",
    "        train_loss.append(loss_avg)\n",
    "        train_accuracy.append(acc)\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Round {epoch+1}: Loss {loss_avg:.4f}, Acc {acc*100:.2f}%\")\n",
    "    \n",
    "    federated_time = time.time() - start_time\n",
    "    print(f\"\\n========== Federated Learning Completed ==========\")\n",
    "    print(f\"Training Time: {federated_time:.2f}s\")\n",
    "\n",
    "    # 사전 훈련 모델 백업\n",
    "    pretrained_model = copy.deepcopy(global_model)\n",
    "\n",
    "    # ===================== 3. Advanced UNGAN Unlearning =====================\n",
    "    unlearn_start_time = time.time()\n",
    "    \n",
    "    # Advanced UNGAN 워크플로우 실행\n",
    "    filtered_images, filtered_labels, trained_generator = advanced_ungan_workflow(\n",
    "        generator, discriminator, full_dataset, unseen_dataset,\n",
    "        retain_idxs, forget_idxs, args, device\n",
    "    )\n",
    "\n",
    "    #  시각화 추가!\n",
    "    visualize_advanced_ungan_results(\n",
    "        trained_generator, discriminator, full_dataset, unseen_dataset,\n",
    "        forget_idxs, filtered_images, filtered_labels, device, args, dataset_type=args.dataset\n",
    "    )\n",
    "\n",
    "    # Hybrid 품질 평가\n",
    "    if len(filtered_images) > 0:\n",
    "        quality_results = evaluate_hybrid_quality(\n",
    "            global_model, filtered_images, filtered_labels,\n",
    "            forget_idxs, unseen_dataset, full_dataset, device\n",
    "        )\n",
    "    else:\n",
    "        quality_results = {}\n",
    "        print(\"Warning: No high-quality synthetic data generated\")\n",
    "\n",
    "    # Synthetic 데이터로 Unlearning 수행\n",
    "    if len(filtered_images) > 0:\n",
    "        print(\"\\n========== Performing Unlearning with Synthetic Data ==========\")\n",
    "        \n",
    "        # Synthetic Dataset 생성\n",
    "        synthetic_dataset = SyntheticImageDataset(filtered_images, filtered_labels)\n",
    "        \n",
    "        # Unlearning 훈련\n",
    "        unlearned_model = copy.deepcopy(pretrained_model)\n",
    "        unlearned_model.train()\n",
    "        \n",
    "        for unlearn_epoch in range(10):  # Unlearning 에포크\n",
    "            local_weights, local_losses = [], []\n",
    "            \n",
    "            # Retain 클라이언트들만 참여\n",
    "            for idx in range(1, args.num_users):\n",
    "                local_model = LocalUpdate(args=args, dataset=full_dataset, idxs=user_groups[idx])\n",
    "                w, loss = local_model.update_weights(\n",
    "                    model=copy.deepcopy(unlearned_model), \n",
    "                    global_round=unlearn_epoch\n",
    "                )\n",
    "                local_weights.append(copy.deepcopy(w))\n",
    "                local_losses.append(loss)\n",
    "            \n",
    "            # Synthetic 데이터로 훈련\n",
    "            synthetic_local = LocalUpdate(args=args, dataset=synthetic_dataset, idxs=None)\n",
    "            w_syn, loss_syn = synthetic_local.update_weights(\n",
    "                model=copy.deepcopy(unlearned_model),\n",
    "                global_round=unlearn_epoch\n",
    "            )\n",
    "            local_weights.append(copy.deepcopy(w_syn))\n",
    "            local_losses.append(loss_syn)\n",
    "            \n",
    "            # 글로벌 모델 업데이트\n",
    "            if local_weights:\n",
    "                unlearned_weights = average_weights(local_weights)\n",
    "                unlearned_model.load_state_dict(unlearned_weights)\n",
    "            \n",
    "            if (unlearn_epoch + 1) % 5 == 0:\n",
    "                loss_avg = sum(local_losses) / len(local_losses)\n",
    "                acc, _ = test_inference(args, unlearned_model, test_dataset)\n",
    "                print(f\"Unlearn Epoch {unlearn_epoch + 1}: Loss {loss_avg:.4f}, Acc {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(\"Skipping unlearning due to insufficient synthetic data\")\n",
    "        unlearned_model = pretrained_model\n",
    "\n",
    "    unlearn_time = time.time() - unlearn_start_time\n",
    "\n",
    "    # ===================== 4. 종합 평가 =====================\n",
    "    print(\"\\n========== Comprehensive Evaluation ==========\")\n",
    "    \n",
    "    # 성능 비교\n",
    "    test_acc_before, test_loss_before = test_inference(args, pretrained_model, test_dataset)\n",
    "    test_acc_after, test_loss_after = test_inference(args, unlearned_model, test_dataset)\n",
    "    \n",
    "    print(f\"[Test Performance]\")\n",
    "    print(f\"  Before: {test_acc_before*100:.2f}% | After: {test_acc_after*100:.2f}%\")\n",
    "    print(f\"  Retention: {(test_acc_after/test_acc_before)*100:.1f}%\")\n",
    "\n",
    "    # MIA 평가\n",
    "    print(f\"\\n[MIA Evaluation]\")\n",
    "    all_idxs = set(range(len(full_dataset)))\n",
    "    non_member_candidates = list(all_idxs - set(forget_idxs))\n",
    "    shadow_idxs = np.random.choice(non_member_candidates, \n",
    "                                 min(len(forget_idxs), len(non_member_candidates)), \n",
    "                                 replace=False)\n",
    "    \n",
    "    mia_before = evaluate_mia(\n",
    "        model=pretrained_model, dataset=full_dataset, test_dataset=test_dataset,\n",
    "        forget_idxs=forget_idxs, retain_idxs=test_idxs, shadow_idxs=shadow_idxs,\n",
    "        device=device, save_path=\"./mia_before_advanced.json\"\n",
    "    )\n",
    "    \n",
    "    mia_after = evaluate_mia(\n",
    "        model=unlearned_model, dataset=full_dataset, test_dataset=test_dataset,\n",
    "        forget_idxs=forget_idxs, retain_idxs=test_idxs, shadow_idxs=shadow_idxs,\n",
    "        device=device, save_path=\"./mia_after_advanced.json\"\n",
    "    )\n",
    "    \n",
    "    print(f\"  Before: {mia_before['auc']:.4f} | After: {mia_after['auc']:.4f}\")\n",
    "    print(f\"  Privacy Gain: {mia_before['auc'] - mia_after['auc']:.4f}\")\n",
    "\n",
    "    # 백도어 ASR 평가\n",
    "    print(f\"\\n[Backdoor ASR Evaluation]\")\n",
    "    target_label = 6\n",
    "    asr_before = evaluate_backdoor_asr(pretrained_model, test_dataset, target_label, device, args.dataset)\n",
    "    asr_after = evaluate_backdoor_asr(unlearned_model, test_dataset, target_label, device, args.dataset)\n",
    "    \n",
    "    print(f\"  Before: {asr_before*100:.2f}% | After: {asr_after*100:.2f}%\")\n",
    "    print(f\"  Attack Reduction: {(asr_before - asr_after)*100:.2f}%\")\n",
    "\n",
    "    # Unseen Dataset 평가\n",
    "    if unseen_dataset is not None:\n",
    "        print(f\"\\n[Unseen Dataset Evaluation]\")\n",
    "        unseen_acc_before, _ = test_inference(args, pretrained_model, unseen_dataset)\n",
    "        unseen_acc_after, _ = test_inference(args, unlearned_model, unseen_dataset)\n",
    "        \n",
    "        print(f\"  Before: {unseen_acc_before*100:.2f}% | After: {unseen_acc_after*100:.2f}%\")\n",
    "        print(f\"  Preservation: {(unseen_acc_after/unseen_acc_before)*100:.1f}%\")\n",
    "\n",
    "    # ===================== 5. 결과 저장 및 요약 =====================\n",
    "    \n",
    "    # 모델 저장\n",
    "    torch.save(pretrained_model.state_dict(), args.save_model)\n",
    "    unlearned_save_path = args.save_model.replace('.pth', '_unlearned.pth')\n",
    "    torch.save(unlearned_model.state_dict(), unlearned_save_path)\n",
    "    \n",
    "    # 종합 결과\n",
    "    total_time = time.time() - start_time\n",
    "    results = {\n",
    "        'experiment_type': 'Advanced_UNGAN_Federated_Unlearning',\n",
    "        'configuration': {\n",
    "            'dataset': args.dataset,\n",
    "            'model': args.model,\n",
    "            'num_users': args.num_users,\n",
    "            'epochs': args.epochs,\n",
    "            'forget_client': forget_client,\n",
    "        },\n",
    "        'timing': {\n",
    "            'federated_learning': federated_time,\n",
    "            'unlearning': unlearn_time,\n",
    "            'total': total_time\n",
    "        },\n",
    "        'performance': {\n",
    "            'test_acc_before': float(test_acc_before),\n",
    "            'test_acc_after': float(test_acc_after),\n",
    "            'retention_rate': float(test_acc_after / test_acc_before)\n",
    "        },\n",
    "        'privacy': {\n",
    "            'mia_auc_before': float(mia_before['auc']),\n",
    "            'mia_auc_after': float(mia_after['auc']),\n",
    "            'privacy_improvement': float(mia_before['auc'] - mia_after['auc'])\n",
    "        },\n",
    "        'security': {\n",
    "            'asr_before': float(asr_before),\n",
    "            'asr_after': float(asr_after),\n",
    "            'attack_mitigation': float(asr_before - asr_after)\n",
    "        },\n",
    "        'synthetic_data': {\n",
    "            'generated_samples': len(filtered_images) if len(filtered_images) > 0 else 0,\n",
    "            'quality_metrics': quality_results\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 결과 JSON 저장\n",
    "    with open('./advanced_ungan_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e07b2b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # ResNet18 + CIFAR-10용 실행 파라미터\n",
    "    import sys\n",
    "    sys.argv = [\n",
    "        'main.py',\n",
    "        '--epochs', '100',         # 실험용으로 줄임\n",
    "        '--num_users', '10',\n",
    "        '--frac', '1.0',\n",
    "        '--local_ep', '10',        # 실험용으로 줄임  \n",
    "        '--local_bs', '64',\n",
    "        '--lr', '0.01',\n",
    "        '--momentum', '0.9',\n",
    "        '--dataset', 'mnist',\n",
    "        '--model', 'cnn',    # ResNet18 사용\n",
    "        '--iid', '1',\n",
    "        '--gpu', '0',\n",
    "        '--num_classes', '10',\n",
    "        '--load_model', 'None',\n",
    "        '--save_model', './saved_models/advanced_cifar_resnet18.pth',\n",
    "        '--z_dim', '100',\n",
    "        '--gen_threshold', '0.7',  # 필터링 임계값 조정\n",
    "        '--num_gen_samples', '128'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72d208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Experiment Settings =====\n",
      "Model           : cnn\n",
      "Dataset         : mnist\n",
      "Num Clients     : 10\n",
      "Fraction        : 1.0\n",
      "IID             : 1\n",
      "Local Epochs    : 10\n",
      "Batch Size      : 64\n",
      "Learning Rate   : 0.01\n",
      "Generator z_dim : 100\n",
      "Disc. Threshold : 0.7\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.91MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 145kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.48MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.57MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 55000 samples\n",
      "Test dataset: 10000 samples\n",
      "Unseen dataset: 5000 samples\n",
      "\n",
      "Model: CNN\n",
      "Forget client: 0\n",
      "Forget data size: 5500\n",
      "Retain data size: 49500\n",
      "\n",
      "========== Starting Federated Learning ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global Training Rounds:   5%|▌         | 5/100 [01:24<26:25, 16.69s/it]"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b47a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from torch.utils.data import ConcatDataset #unseen dataë¥¼ í†µí•´ì„œ ì–¸ëŸ¬ë‹ ì¬í•™ìŠµì—ì„œ ì‚¬ìš©.\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference\n",
    "from models import CNNMnist,ResNet18, CNNCifar, Generator, Discriminator, generate_images, filter_images\n",
    "from utils import get_dataset, average_weights, exp_details, create_poisoned_dataset\n",
    "from unlearn import (\n",
    "    train_generator_ungan,\n",
    "    train_gd_ungan,\n",
    "    SyntheticImageDataset, \n",
    "    partition_synthetic_data_iid,\n",
    "    partition_synthetic_data_dirichlet,\n",
    "    get_synthetic_subset\n",
    ")\n",
    "from evaluate_mia import evaluate_mia, comprehensive_evaluation, evaluate_synthetic_classification_accuracy, evaluate_classification_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe0131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "\n",
    "def visualize_real_vs_generated_with_filtering(generator, discriminator, dataset, forget_idxs, \n",
    "                                             z_dim=100, device='cpu', threshold=0.5, num_samples=16):\n",
    "    \"\"\"\n",
    "    ì‹¤ì œ ì´ë¯¸ì§€, ìƒì„±ëœ ì´ë¯¸ì§€, í•„í„°ë§ëœ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ë¹„êµ ì‹œê°í™”\n",
    "    \"\"\"\n",
    "    # ì‹¤ì œ ì´ë¯¸ì§€ ìƒ˜í”Œë§\n",
    "    real_images = []\n",
    "    sample_idxs = np.random.choice(forget_idxs, min(num_samples, len(forget_idxs)), replace=False)\n",
    "    \n",
    "    for idx in sample_idxs:\n",
    "        img, _ = dataset[idx]\n",
    "        real_images.append(img)\n",
    "    \n",
    "    real_images = torch.stack(real_images)\n",
    "    \n",
    "    # ë” ë§ì€ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ì„œ í•„í„°ë§ íš¨ê³¼ë¥¼ ë³´ì—¬ì£¼ê¸°\n",
    "    num_generate = num_samples * 2  # 2ë°° ìƒì„±í•´ì„œ í•„í„°ë§\n",
    "    \n",
    "    # ìƒì„±ëœ ì´ë¯¸ì§€\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(num_generate, z_dim, device=device)\n",
    "        generated_images = generator(noise)\n",
    "        \n",
    "        # Discriminatorë¡œ í’ˆì§ˆ í‰ê°€\n",
    "        d_scores = discriminator(generated_images.to(device))\n",
    "        \n",
    "        # CPUë¡œ ì´ë™í•˜ê³  ì •ê·œí™”\n",
    "        generated_images = generated_images.cpu()\n",
    "        generated_images = torch.clamp(generated_images, -1, 1)\n",
    "        generated_images = (generated_images + 1) / 2\n",
    "        \n",
    "        # í•„í„°ë§ (threshold ì´ìƒë§Œ ì„ íƒ)\n",
    "        d_scores = d_scores.cpu().squeeze()\n",
    "        high_quality_mask = d_scores > threshold\n",
    "        \n",
    "        if high_quality_mask.sum() > 0:\n",
    "            filtered_images = generated_images[high_quality_mask]\n",
    "            filtered_scores = d_scores[high_quality_mask]\n",
    "            # ìƒìœ„ num_samplesê°œë§Œ ì„ íƒ\n",
    "            if len(filtered_images) > num_samples:\n",
    "                top_indices = torch.topk(filtered_scores, num_samples)[1]\n",
    "                filtered_images = filtered_images[top_indices]\n",
    "        else:\n",
    "            # í•„í„°ë§ëœ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ìƒìœ„ ì ìˆ˜ ì´ë¯¸ì§€ë“¤ ì„ íƒ\n",
    "            top_indices = torch.topk(d_scores, num_samples)[1]\n",
    "            filtered_images = generated_images[top_indices]\n",
    "    \n",
    "    # ì‹¤ì œ ì´ë¯¸ì§€ ì •ê·œí™”\n",
    "    if real_images.min() < 0:  # ì´ë¯¸ ì •ê·œí™”ëœ ê²½ìš°\n",
    "        real_images = (real_images + 1) / 2\n",
    "    elif real_images.max() > 1:  # 0-255 ë²”ìœ„\n",
    "        real_images = real_images / 255.0\n",
    "    \n",
    "    # 3í–‰ìœ¼ë¡œ ë¹„êµ ì‹œê°í™”\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 9))\n",
    "    \n",
    "    # ì‹¤ì œ ì´ë¯¸ì§€\n",
    "    real_grid = vutils.make_grid(real_images, nrow=8, normalize=False, padding=2)\n",
    "    real_grid_np = real_grid.permute(1, 2, 0).numpy()\n",
    "    ax1.imshow(real_grid_np, cmap='gray' if real_images.shape[1] == 1 else None)\n",
    "    ax1.set_title('Real Images (Forget Set)', fontsize=14)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # ëª¨ë“  ìƒì„±ëœ ì´ë¯¸ì§€ (ì²˜ìŒ num_samplesê°œ)\n",
    "    all_gen_grid = vutils.make_grid(generated_images[:num_samples], nrow=8, normalize=False, padding=2)\n",
    "    all_gen_grid_np = all_gen_grid.permute(1, 2, 0).numpy()\n",
    "    ax2.imshow(all_gen_grid_np, cmap='gray' if generated_images.shape[1] == 1 else None)\n",
    "    ax2.set_title('All Generated Images', fontsize=14)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # í•„í„°ë§ëœ ê³ í’ˆì§ˆ ì´ë¯¸ì§€\n",
    "    filtered_grid = vutils.make_grid(filtered_images, nrow=8, normalize=False, padding=2)\n",
    "    filtered_grid_np = filtered_grid.permute(1, 2, 0).numpy()\n",
    "    ax3.imshow(filtered_grid_np, cmap='gray' if filtered_images.shape[1] == 1 else None)\n",
    "    ax3.set_title(f'High-Quality Filtered Images (D-score > {threshold})', fontsize=14)\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./real_vs_generated_filtered.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Discriminator ì ìˆ˜ í†µê³„ ì¶œë ¥\n",
    "    print(f\"Discriminator Scores - Mean: {d_scores.mean():.3f}, Std: {d_scores.std():.3f}\")\n",
    "    print(f\"High-quality images (>{threshold}): {high_quality_mask.sum().item()}/{len(d_scores)}\")\n",
    "    print(\"Comparison image saved to: ./real_vs_generated_filtered.png\")\n",
    "\n",
    "\n",
    "def visualize_discriminator_scores(generator, discriminator, z_dim=100, device='cpu', num_samples=100):\n",
    "    \"\"\"\n",
    "    Discriminator ì ìˆ˜ ë¶„í¬ë¥¼ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ì‹œê°í™”\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(num_samples, z_dim, device=device)\n",
    "        generated_images = generator(noise)\n",
    "        d_scores = discriminator(generated_images).cpu().squeeze().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(d_scores, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "    plt.xlabel('Discriminator Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Discriminator Scores for Generated Images')\n",
    "    plt.axvline(x=0.5, color='red', linestyle='--', label='Threshold (0.5)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('./discriminator_scores_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Discriminator scores - Min: {d_scores.min():.3f}, Max: {d_scores.max():.3f}\")\n",
    "    print(f\"Mean: {d_scores.mean():.3f}, Std: {d_scores.std():.3f}\")\n",
    "    print(\"Discriminator scores distribution saved to: ./discriminator_scores_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30ef2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_dataset_to_device(dataset, device):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for x, y in dataset:\n",
    "        images.append(x.to(device))\n",
    "        labels.append(torch.tensor(y).to(device))\n",
    "    return TensorDataset(torch.stack(images), torch.stack(labels))\n",
    "\n",
    "\n",
    "def add_backdoor_trigger(x):\n",
    "    x_bd = x.clone()\n",
    "    x_bd[:, 25:28, 25:28] = 0.9\n",
    "    return x_bd\n",
    "\n",
    "def evaluate_backdoor_asr(model, dataset, target_label, device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            x, y = dataset[i]\n",
    "            # ë°±ë„ì–´ íŠ¸ë¦¬ê±° ì‚½ì…\n",
    "            x_bd = add_backdoor_trigger(x).to(device)\n",
    "            x_bd = x_bd.unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€\n",
    "\n",
    "            output = model(x_bd)\n",
    "            pred = output.argmax(dim=1).item()\n",
    "\n",
    "            total += 1\n",
    "            if pred == target_label:\n",
    "                correct += 1\n",
    "\n",
    "    asr = correct / total\n",
    "    return asr\n",
    "\n",
    "def select_model(args, train_dataset):\n",
    "    if args.model == 'cnn':\n",
    "        if args.dataset == 'cifar':\n",
    "            return CNNCifar(args=args)  # CIFAR-10ìš© CNN ì¶”ê°€ í•„ìš”\n",
    "        else:\n",
    "            return CNNMnist(args=args)  # MNISTìš© CNN\n",
    "    elif args.model == 'resnet':\n",
    "        return ResNet18(num_classes=args.num_classes)  # CIFAR-10ìš© ResNet\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b712b",
   "metadata": {},
   "source": [
    "# ì „ì²´ í”Œë¡œìš°\n",
    "\n",
    "## 1. ë°ì´í„° ì¤€ë¹„\n",
    "CIFAR-10 ë°ì´í„°ì…‹\n",
    "Training Set (55,000) â†’ Non-IIDë¡œ 10ê°œ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ë¶„ë°°\n",
    "\n",
    "Test Set (10,000) â†’ ì„±ëŠ¥ í‰ê°€ìš©\n",
    "\n",
    "Unseen Set (5,000) â†’ ëª¨ë¸ì´ í•™ìŠµí•˜ì§€ ì•Šì€ ê¹¨ë—í•œ ë°ì´í„°\n",
    "\n",
    "## 2. ì´ˆê¸° ì—°í•©í•™ìŠµ + ì–¸ëŸ¬ë‹ ìš”ì²­ ì²˜ë¦¬\n",
    "í´ë¼ì´ì–¸íŠ¸ 0~9 ëª¨ë‘ ì°¸ì—¬\n",
    "\n",
    "ì¤‘ê°„ì— í´ë¼ì´ì–¸íŠ¸ 0ì´ ì–¸ëŸ¬ë‹ ìš”ì²­ ğŸ“¢\n",
    "\n",
    "ì„œë²„: \"í´ë¼ì´ì–¸íŠ¸ 0 ì œì™¸í•˜ê³  ê³„ì† ì§„í–‰\"\n",
    "    \n",
    "ê²°ê³¼: forget ë°ì´í„° ì˜í–¥ì´ ì œê±°ëœ ëª¨ë¸ (í•˜ì§€ë§Œ ì„±ëŠ¥ í•˜ë½í•˜ëŠ” ë¬¸ì œ)\n",
    "\n",
    "## 3. GANìœ¼ë¡œ ì—°í•©í•™ìŠµ + ì–¸ëŸ¬ë‹ ìš”ì²­ ì²˜ë¦¬\n",
    "ëª©í‘œ: forget ë°ì´í„°ë¥¼ ê³ í’ˆì§ˆë¡œ ëŒ€ì²´í•  í•©ì„± ë°ì´í„° ìƒì„±\n",
    "\n",
    "Generator í›ˆë ¨: forget ë°ì´í„° íŠ¹ì§• í•™ìŠµ\n",
    "\n",
    "Discriminator í›ˆë ¨: ì§„ì§œ/ê°€ì§œ êµ¬ë¶„\n",
    "\n",
    "Unseen ë°ì´í„° í™œìš©: ë” ìì—°ìŠ¤ëŸ¬ìš´ ìƒì„±\n",
    "\n",
    "ê²°ê³¼: forget_sizeë§Œí¼ì˜ ê³ í’ˆì§ˆ í•©ì„± ë°ì´í„°\n",
    "\n",
    "## 4. ì„±ëŠ¥ ë³µêµ¬ ì—°í•©í•™ìŠµ\n",
    "ë°ì´í„° êµ¬ì„±:\n",
    "\n",
    "Retain Set (ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ 1~9ì˜ ë°ì´í„°)\n",
    "\n",
    "Synthetic Set (ìƒì„±ëœ í•©ì„± ë°ì´í„°)\n",
    "\n",
    "ì—°í•©í•™ìŠµ ì°¸ì—¬ì:\n",
    "\n",
    "í´ë¼ì´ì–¸íŠ¸ 1~9 (forget í´ë¼ì´ì–¸íŠ¸ 0ì€ ì—¬ì „íˆ ì œì™¸)\n",
    "\n",
    "ê° í´ë¼ì´ì–¸íŠ¸: ê¸°ì¡´ retain ë°ì´í„° + í• ë‹¹ëœ í•©ì„± ë°ì´í„°\n",
    "\n",
    "ê²°ê³¼: ì„±ëŠ¥ì´ ë³µêµ¬ëœ ì–¸ëŸ¬ë‹ ëª¨ë¸\n",
    "\n",
    "## 5. ì„±ëŠ¥ ë¹„êµí‰ê°€\n",
    "4ê°€ì§€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„±ëŠ¥ ì¸¡ì •:\n",
    "\n",
    "Original Test Set: ì „ì²´ì ì¸ ëª¨ë¸ ì„±ëŠ¥\n",
    "\n",
    "Retain Set: ë‚¨ì•„ìˆëŠ” ë°ì´í„° ì„±ëŠ¥ \n",
    "\n",
    "Forget Set: ìŠí˜€ì§„ ë°ì´í„° ì„±ëŠ¥ \n",
    "\n",
    "Synthetic Set: ìƒì„± ë°ì´í„° í’ˆì§ˆ\n",
    "\n",
    "í•µì‹¬: Synthetic vs Unseen ì„±ëŠ¥ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39658942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    args = args_parser()\n",
    "    device = 'cuda' if args.gpu and torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    exp_details(args)\n",
    "\n",
    "    # ===================== 1. ë°ì´í„°ì…‹ ë¡œë”© ë° ì´ˆê¸°í™” =====================\n",
    "    train_dataset, test_dataset, unseen_dataset, user_groups = get_dataset(args)\n",
    "    \n",
    "\n",
    "    # full_dataset, user_groups = create_poisoned_dataset(train_dataset, user_groups, args,\n",
    "    #                                                     malicious_client=0,\n",
    "    #                                                     target_label=6,\n",
    "    #                                                     poison_ratio=0.8)\n",
    "\n",
    "    full_dataset = train_dataset  # create_poisoned_dataset ì œê±°\n",
    "\n",
    "\n",
    "\n",
    "    global_model = select_model(args, full_dataset).to(device)\n",
    "    global_model.train()\n",
    "\n",
    "    generator = Generator(z_dim=args.z_dim).to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "\n",
    "    global_weights = global_model.state_dict()\n",
    "    train_loss, train_accuracy = [], []\n",
    "\n",
    "    forget_client = 0\n",
    "    forget_idxs = user_groups[forget_client]\n",
    "    retain_idxs = [i for i in range(len(train_dataset)) if i not in forget_idxs]\n",
    "    test_idxs = np.random.choice(len(test_dataset), len(forget_idxs), replace=False)\n",
    "\n",
    "    # ë°ì´í„° ìˆ˜ëŸ‰ ê· í˜• - unseen ë°ì´í„° ì¤€ë¹„\n",
    "    forget_size = len(forget_idxs)\n",
    "    unseen_idxs = np.random.choice(len(unseen_dataset), forget_size, replace=False)\n",
    "    unseen_subset = Subset(unseen_dataset, unseen_idxs)\n",
    "    \n",
    "    print(f\"Data sizes - Forget: {forget_size}, Unseen: {len(unseen_idxs)}\")\n",
    "\n",
    "\n",
    "    # ===================== 2. ì—°í•© í•™ìŠµ (FedAvg) =====================\n",
    "    for epoch in tqdm(range(args.epochs), desc='Global Training Rounds'):\n",
    "        print(f'\\n| Global Training Round : {epoch + 1} |')\n",
    "\n",
    "        local_weights, local_losses = [], []\n",
    "        m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "        for idx in idxs_users:\n",
    "            if idx == forget_client:\n",
    "                continue  # ì–¸ëŸ¬ë‹ ìš”ì²­ì ì œê±° / ì¬í•™ìŠµ\n",
    "\n",
    "            local_model = LocalUpdate(args=args, dataset=full_dataset, idxs=user_groups[idx])\n",
    "            w, loss = local_model.update_weights(model=copy.deepcopy(global_model), global_round=epoch)\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(loss)\n",
    "\n",
    "        global_weights = average_weights(local_weights)\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        loss_avg = sum(local_losses) / len(local_losses)\n",
    "        acc, _ = test_inference(args, global_model, test_dataset)\n",
    "        train_loss.append(loss_avg)\n",
    "        train_accuracy.append(acc)\n",
    "\n",
    "        print(f\"Training Loss: {loss_avg:.4f} | Train Accuracy: {acc*100:.2f}%\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"Re_Training Time: {elapsed_time:.2f}ì´ˆ\")\n",
    "    test_acc_before, test_loss_before = test_inference(args, global_model, test_dataset)\n",
    "    print(f\"\\n[Test Before Unlearning] Accuracy: {test_acc_before*100:.2f}% | Loss: {test_loss_before:.4f}\")\n",
    "    \n",
    "\n",
    "    # ===================== 3. ì–¸ëŸ¬ë‹ ì „ ë¶„ë¥˜ ì •í™•ë„ í‰ê°€ =====================\n",
    "    print(\"\\n========== Before Unlearning - Classification Performance ===========\")\n",
    "    before_results = comprehensive_evaluation(\n",
    "        model=global_model,\n",
    "        train_dataset=full_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        forget_idxs=forget_idxs,\n",
    "        retain_idxs=retain_idxs,\n",
    "        synthetic_dataset=None,  # ì•„ì§ ìƒì„± ì „\n",
    "        device=device,\n",
    "        save_path=\"./classification_results_before_unlearning.json\"\n",
    "    )\n",
    "\n",
    "    # Unseen ë°ì´í„° ì„±ëŠ¥ë„ ì¸¡ì • (ë¹„êµ ê¸°ì¤€)\n",
    "    unseen_acc_before = evaluate_synthetic_classification_accuracy(\n",
    "        global_model, unseen_subset, device, \"Unseen Data (Before Unlearning)\"\n",
    "    )\n",
    "    \n",
    "     \n",
    "    # ===================== 4. MIA í‰ê°€ =====================\n",
    "    print(\"[MIA] Evaluating membership inference attack...\")\n",
    "\n",
    "    all_idxs = set(range(len(full_dataset)))\n",
    "    non_member_candidates = list(all_idxs - set(forget_idxs))\n",
    "    #ì—¬ê¸°ì—ì„œ ì‰ë„ìš°ì—ëŠ” forget ë°ì´í„°ê°€ ì—†ë„ë¡ í•˜ê¸°.\n",
    "    mia_result = evaluate_mia(\n",
    "        model=global_model,\n",
    "        dataset=full_dataset,\n",
    "        test_dataset= test_dataset,\n",
    "        forget_idxs=forget_idxs,\n",
    "        retain_idxs=test_idxs,\n",
    "        shadow_idxs=np.random.choice(non_member_candidates, len(forget_idxs), replace=False),\n",
    "        device=device,\n",
    "        save_path=\"./mia_result_before.json\"\n",
    "    )\n",
    "\n",
    "    print(f\"[MIA] AUC: {mia_result['auc']:.4f}\")\n",
    "\n",
    "\n",
    "    # ===================== 5. í•©ì„± ë°ì´í„° ìƒì„± ë° ì‹œê°í™” =====================\n",
    "    print(\"\\n[Generating synthetic data...]\")\n",
    "    \n",
    "    # Generatorì™€ Discriminator í•¨ê»˜ í›ˆë ¨ (ë” ê¸´ ì—í¬í¬)\n",
    "    generator, discriminator = train_gd_ungan(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        dataset=full_dataset,\n",
    "        retain_idxs=retain_idxs,\n",
    "        forget_idxs=forget_idxs,\n",
    "        device=device,\n",
    "        lambda_adv=1.0,\n",
    "        z_dim=args.z_dim,\n",
    "        batch_size=64,\n",
    "        epochs=30  # GAN í’ˆì§ˆ í–¥ìƒ\n",
    "    )\n",
    "    \n",
    "    # í•©ì„± ë°ì´í„° ìƒì„±\n",
    "    synthetic_images, synthetic_labels = generate_images(\n",
    "        generator=generator,\n",
    "        idxs=forget_idxs,\n",
    "        dataset=full_dataset,\n",
    "        device=device,\n",
    "        z_dim=args.z_dim,\n",
    "        num_generate=forget_size\n",
    "    )\n",
    "    \n",
    "    # Discriminatorë¡œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ í•„í„°ë§\n",
    "    print(\"[Filtering synthetic images with Discriminator...]\")\n",
    "    filtered_images, filtered_labels = filter_images(\n",
    "        discriminator=discriminator,\n",
    "        images=synthetic_images,\n",
    "        labels=synthetic_labels,\n",
    "        threshold=args.gen_threshold,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated {len(synthetic_images)} images, filtered to {len(filtered_images)} high-quality images\")\n",
    "    \n",
    "    # í•„í„°ë§ëœ ì´ë¯¸ì§€ë¡œ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    if len(filtered_images) > 0:\n",
    "        synthetic_dataset = SyntheticImageDataset(filtered_images, filtered_labels)\n",
    "        use_filtered = True\n",
    "    else:\n",
    "        print(\"[Warning] No images passed discriminator filter, using all generated images\")\n",
    "        synthetic_dataset = SyntheticImageDataset(synthetic_images, synthetic_labels)\n",
    "        use_filtered = False\n",
    "    \n",
    "    # ì‹œê°í™” (í•„í„°ë§ëœ ì´ë¯¸ì§€ ì‚¬ìš©)\n",
    "    print(\"[Visualizing real vs synthetic images...]\")\n",
    "    visualize_real_vs_generated_with_filtering(\n",
    "        generator, discriminator, full_dataset, forget_idxs, \n",
    "        args.z_dim, device, args.gen_threshold\n",
    "    )\n",
    "\n",
    "    # ===================== 6. ì–¸ëŸ¬ë‹ ê³¼ì •: í•©ì„± ë°ì´í„°ë¡œ ì¶”ê°€ ì—°í•©í•™ìŠµ =====================\n",
    "    print(\"\\n[Unlearning Process: Federated Learning with Synthetic Data...]\")\n",
    "    \n",
    "    # Retain ë°ì´í„°ì™€ í•©ì„± ë°ì´í„°ë¥¼ í•©ì¹œ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    retain_dataset = Subset(full_dataset, retain_idxs)\n",
    "    combined_dataset = ConcatDataset([retain_dataset, synthetic_dataset])\n",
    "    \n",
    "    print(f\"Combined dataset size: {len(combined_dataset)} (Retain: {len(retain_dataset)}, Synthetic: {len(synthetic_dataset)})\")\n",
    "    \n",
    "    # ìƒˆë¡œìš´ user_groups êµ¬ì„± (forget_client ì œì™¸)\n",
    "    remaining_clients = [i for i in range(args.num_users) if i != forget_client]\n",
    "    \n",
    "    # Retain ë°ì´í„°ë¥¼ ë‚¨ì€ í´ë¼ì´ì–¸íŠ¸ë“¤ì—ê²Œ ì¬ë¶„ë°°\n",
    "    retain_user_groups = {}\n",
    "    for client_id in remaining_clients:\n",
    "        retain_user_groups[client_id] = user_groups[client_id]  # ê¸°ì¡´ retain ë°ì´í„° ìœ ì§€\n",
    "    \n",
    "    # í•©ì„± ë°ì´í„°ë„ Non-IIDë¡œ ë¶„ë°° (Dirichlet ì‚¬ìš©)\n",
    "    from utils import partition_data_dirichlet\n",
    "\n",
    "    # í•©ì„± ë°ì´í„°ë¥¼ Non-IIDë¡œ ë¶„ë°°\n",
    "    synthetic_labels = [synthetic_dataset[i][1] for i in range(len(synthetic_dataset))]\n",
    "    synthetic_user_groups = partition_synthetic_data_dirichlet(\n",
    "        synthetic_dataset, \n",
    "        num_users=len(remaining_clients), \n",
    "        alpha=args.alpha,  # ë™ì¼í•œ alpha ì‚¬ìš©\n",
    "        num_classes=args.num_classes\n",
    "    )\n",
    "\n",
    "    # ì–¸ëŸ¬ë‹ ì—°í•©í•™ìŠµ\n",
    "    unlearning_epochs = args.epochs  # ì ˆë°˜ ì—í¬í¬\n",
    "    \n",
    "    for epoch in tqdm(range(unlearning_epochs), desc='Unlearning FL Rounds'):\n",
    "        print(f'\\n| Unlearning FL Round : {epoch + 1} |')\n",
    "        \n",
    "        local_weights, local_losses = [], []\n",
    "        m = max(int(args.frac * len(remaining_clients)), 1)\n",
    "        idxs_users = np.random.choice(remaining_clients, m, replace=False)\n",
    "        \n",
    "        for idx in idxs_users:\n",
    "            # Retain ë°ì´í„° ì¸ë±ìŠ¤\n",
    "            retain_client_idxs = retain_user_groups[idx]\n",
    "            \n",
    "            # í•©ì„± ë°ì´í„° ì¸ë±ìŠ¤ (combined_datasetì—ì„œì˜ ìœ„ì¹˜ ì¡°ì •)\n",
    "            synthetic_client_idx = remaining_clients.index(idx)\n",
    "            synthetic_client_idxs = synthetic_user_groups[synthetic_client_idx]\n",
    "            synthetic_adjusted_idxs = [i + len(retain_dataset) for i in synthetic_client_idxs]\n",
    "            \n",
    "            # í´ë¼ì´ì–¸íŠ¸ì˜ ì „ì²´ ë°ì´í„° ì¸ë±ìŠ¤ (retain + synthetic)\n",
    "            client_all_idxs = list(retain_client_idxs) + synthetic_adjusted_idxs\n",
    "            \n",
    "            # ë¡œì»¬ ì—…ë°ì´íŠ¸\n",
    "            local_model = LocalUpdate(args=args, dataset=combined_dataset, idxs=client_all_idxs)\n",
    "            w, loss = local_model.update_weights(model=copy.deepcopy(global_model), global_round=epoch)\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(loss)\n",
    "        \n",
    "        # ê¸€ë¡œë²Œ ëª¨ë¸ ì—…ë°ì´íŠ¸\n",
    "        global_weights = average_weights(local_weights)\n",
    "        global_model.load_state_dict(global_weights)\n",
    "        \n",
    "        loss_avg = sum(local_losses) / len(local_losses)\n",
    "        acc, _ = test_inference(args, global_model, test_dataset)\n",
    "        \n",
    "        print(f\"Unlearning FL Loss: {loss_avg:.4f} | Test Accuracy: {acc*100:.2f}%\")\n",
    "    \n",
    "    print(\"Unlearning process completed!\")\n",
    "\n",
    "\n",
    "    # ===================== 7. ì–¸ëŸ¬ë‹ í›„ ë¶„ë¥˜ ì •í™•ë„ í‰ê°€ =====================\n",
    "    print(\"\\n========== After Unlearning - Classification Performance ===========\")\n",
    "    after_results = comprehensive_evaluation(\n",
    "        model=global_model,\n",
    "        train_dataset=full_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        forget_idxs=forget_idxs,\n",
    "        retain_idxs=retain_idxs,\n",
    "        synthetic_dataset=None,  # ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´ None\n",
    "        device=device,\n",
    "        save_path=\"./classification_results_after_unlearning.json\"\n",
    "    )\n",
    "    \n",
    "    # ì–¸ëŸ¬ë‹ í›„ í•©ì„± ë°ì´í„° ì„±ëŠ¥ ì¸¡ì •\n",
    "    synthetic_acc_after = evaluate_synthetic_classification_accuracy(\n",
    "        global_model, synthetic_dataset, device, \"Synthetic Data (After Unlearning)\"\n",
    "    )\n",
    "    \n",
    "    # ì–¸ëŸ¬ë‹ í›„ Unseen ë°ì´í„° ì„±ëŠ¥ ì¸¡ì • (ì´ìƒì  ê¸°ì¤€)\n",
    "    unseen_acc_after = evaluate_synthetic_classification_accuracy(\n",
    "        global_model, unseen_subset, device, \"Unseen Data (After Unlearning)\"\n",
    "    )\n",
    "    \n",
    "    # ===================== 8. ì–¸ëŸ¬ë‹ í›„ MIA í‰ê°€ =====================\n",
    "    print(\"\\n[MIA] Evaluating membership inference attack after unlearning...\")\n",
    "    mia_result_after = evaluate_mia(\n",
    "        model=global_model,\n",
    "        dataset=full_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        forget_idxs=forget_idxs,\n",
    "        retain_idxs=test_idxs,\n",
    "        shadow_idxs=np.random.choice(non_member_candidates, len(forget_idxs), replace=False),\n",
    "        device=device,\n",
    "        save_path=\"./mia_result_after.json\"\n",
    "    )\n",
    "    print(f\"[MIA After] AUC: {mia_result_after['auc']:.4f}\")\n",
    "    \n",
    "    # ===================== 9. ê²°ê³¼ ë¹„êµ ìš”ì•½ =====================\n",
    "    print(\"\\n========== Classification Performance Comparison ===========\")\n",
    "    print(f\"Original Test Set - Before: {before_results['original_test_accuracy']*100:.2f}% | After: {after_results['original_test_accuracy']*100:.2f}%\")\n",
    "    print(f\"Retain Set - Before: {before_results['retain_set_accuracy']*100:.2f}% | After: {after_results['retain_set_accuracy']*100:.2f}%\")\n",
    "    print(f\"Forget Set - Before: {before_results['forget_set_accuracy']*100:.2f}% | After: {after_results['forget_set_accuracy']*100:.2f}%\")\n",
    "    print(\"============================================================\")\n",
    "    \n",
    "    print(\"\\n========== Replacement Data Quality Comparison ===========\")\n",
    "    print(f\"Unseen Data - Before: {unseen_acc_before*100:.2f}% | After: {unseen_acc_after*100:.2f}%\")\n",
    "    print(f\"Synthetic Data - After: {synthetic_acc_after*100:.2f}%\")\n",
    "    print(f\"Quality Gap (Unseen vs Synthetic): {abs(unseen_acc_after - synthetic_acc_after)*100:.2f}%\")\n",
    "    print(\"============================================================\")\n",
    "    \n",
    "    print(\"\\n========== MIA Attack Comparison ===========\")\n",
    "    print(f\"MIA AUC - Before: {mia_result['auc']:.4f} | After: {mia_result_after['auc']:.4f}\")\n",
    "    print(f\"Privacy Improvement: {(mia_result['auc'] - mia_result_after['auc']):.4f}\")\n",
    "    print(\"============================================================\")\n",
    "    \n",
    "\n",
    "\n",
    "    #torch.save(global_model.state_dict(), args.save_model)\n",
    "    #print(f\"[Saved] model to {args.save_model}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3498a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.argv = [\n",
    "    'ipykernel_launcher.py',\n",
    "    '--epochs', '50',\n",
    "    '--num_users', '10',\n",
    "    '--frac', '1.0',\n",
    "    '--local_ep', '10',\n",
    "    '--local_bs', '64',\n",
    "    '--lr', '0.01',\n",
    "    '--momentum', '0.9',\n",
    "    '--dataset', 'cifar',\n",
    "    '--model', 'resnet',\n",
    "    '--iid', '0',\n",
    "    '--gpu', '0',\n",
    "    '--num_classes', '10',\n",
    "    '--dirichlet', '1',  # ê°’ ì¶”ê°€\n",
    "    '--alpha', '0.3',\n",
    "    '--load_model', 'None',\n",
    "    '--save_model', './saved_models/model.pth',\n",
    "    '--z_dim', '100',\n",
    "    '--gen_threshold', '0.5',\n",
    "    '--num_gen_samples', '128'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f902c",
   "metadata": {},
   "source": [
    "# êµìˆ˜ë‹˜ ì „ë‹¬ì‚¬í•­ ë°˜ì˜\n",
    "\n",
    "1. ë°ì´í„°ì…‹ì„ cifar-10ìœ¼ë¡œ ì§„í–‰í•  ê²ƒ --> MNISTëŠ” ë„ˆë¬´ ë‹¨ìˆœí•´ì„œ GAN ì„±ëŠ¥ í‰ê°€ê°€ ì–´ë ¤ì›€\n",
    "2. Non-IID í™˜ê²½ìœ¼ë¡œ ì§„í–‰í•  ê²ƒ. ì´ë•Œ ë’¤ë¦¬ëŒë ˆ ë¶„í¬ë¥¼ ì‚¬ìš©í•  ê²ƒ\n",
    "3. GANì˜ ì„±ëŠ¥ í–¥ìƒì´ í•„ìš”í•¨\n",
    "4. ë°ì´í„° ì„¸íŠ¸ì˜ ìˆ˜ë¥¼ ë§ì¶”ì. unseenì„ 1000ê°œ ì¼ë‹¤ë©´ unseen + forgetë„ 1000ê°œ ì´ëŸ° ì‹ìœ¼ë¡œ!\n",
    "5. unseen ë°ì´í„°ì™€ ë¹„êµí•´ì•¼ í•¨. ì¦‰ í•©ì„± ë°ì´í„°ê°€ unseen ë°ì´í„°ì™€ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ”ê°€? \n",
    "6. ê¸°ì¡´ ì—°í•©í•™ìŠµì—ì„œ ì‚¬ìš©í•œ ì „ì²´ ë°ì´í„°, retain ë°ì´í„°, forget ë°ì´í„°, ìƒì„±í•œ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ë¶„ë¥˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ê±¸ ì¶”ê°€\n",
    "\n",
    "\n",
    "==> ì¦‰, ìš°ë¦¬ëŠ” ë™ì¼í•œ ì¡°ê±´ì—ì„œ ëª¨ë“  ë°ì´í„°ë¥¼ í‰ê°€í•´ì•¼í•˜ê³  ê°™ì€ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼í•¨. Unseenê³¼ ìƒì„± ë°ì´í„°ì˜ í’ˆì§ˆ ê²©ì°¨ê°€ ì ì„ìˆ˜ë¡ ì¢‹ìŒ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f8646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Experiment Settings =====\n",
      "Model           : resnet\n",
      "Dataset         : cifar\n",
      "Num Clients     : 10\n",
      "Fraction        : 1.0\n",
      "IID             : 0\n",
      "Local Epochs    : 10\n",
      "Batch Size      : 64\n",
      "Learning Rate   : 0.01\n",
      "Generator z_dim : 100\n",
      "Disc. Threshold : 0.5\n",
      "===============================\n",
      "Data sizes - Forget: 4981, Unseen: 4981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global Training Rounds:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Global Training Round : 1 |\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

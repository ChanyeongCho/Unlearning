{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e47e5ba1",
   "metadata": {},
   "source": [
    "# 연합학습 환경에서의 머신 언러닝 기법\n",
    "\n",
    "1. 언러닝의 목표가 무엇인가? \n",
    ": 서버를 속이는 것인가? 클라이언트를 속이는 것인가? 아니면 외부 공격자를 속이는 것인가\n",
    "\n",
    "2. 데이터를 생성하는 건 좋은데 이걸 언러닝 목표에 맞게 어떻게 활용할 것인가?\n",
    ": retrain 했을 때와 비슷한 성능을 지니면서, 시간적으로 효율적이고, 다른 언러닝 기법과 비교했을 때 성능이 보장되어야함\n",
    "\n",
    "3. 항상 default 값으로 같이 비교할 수치\n",
    ": original 모델(언러닝을 진행하지 않은 연합학습 모델), retrain 모델(언러닝 요청이 들어와 언러닝 클라이언트를 제외하고 나머지 클라이언트만 다시 학습), fine-tune 모델 (데이터 생성을 통한 언러닝 적용)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b47a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt     \n",
    "import torchvision.utils as vutils \n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset #unseen data를 통해서 언러닝 재학습에서 사용.\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference\n",
    "from models import CNNMnist,ResNet18, CNNCifar, Generator, Discriminator, generate_images, filter_images\n",
    "from utils import get_dataset, average_weights, exp_details, create_poisoned_dataset, generate_fixed_threshold_data\n",
    "from unlearn import (\n",
    "    train_generator_ungan,\n",
    "    train_gd_ungan,\n",
    "    train_gd_ungan_with_unseen,\n",
    "    train_gd_ungan_unseen_only, \n",
    "    train_gd_ungan_forget_only,\n",
    "    SyntheticImageDataset, \n",
    "    partition_synthetic_data_iid,\n",
    "    partition_synthetic_data_dirichlet,\n",
    "    get_synthetic_subset\n",
    ")\n",
    "from evaluate_mia import evaluate_mia, comprehensive_evaluation, evaluate_synthetic_classification_accuracy, evaluate_classification_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe0131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_real_vs_generated_with_filtering(generator, discriminator, dataset, forget_idxs, \n",
    "                                             z_dim=100, device='cpu', threshold=0.5, num_samples=16):\n",
    "    \"\"\"\n",
    "    실제 이미지, 생성된 이미지, 필터링된 이미지를 모두 비교 시각화\n",
    "    \"\"\"\n",
    "    # 실제 이미지 샘플링\n",
    "    real_images = []\n",
    "    sample_idxs = np.random.choice(forget_idxs, min(num_samples, len(forget_idxs)), replace=False)\n",
    "    \n",
    "    for idx in sample_idxs:\n",
    "        img, _ = dataset[idx]\n",
    "        real_images.append(img)\n",
    "    \n",
    "    real_images = torch.stack(real_images)\n",
    "    \n",
    "    # 더 많은 이미지를 생성해서 필터링 효과를 보여주기\n",
    "    num_generate = num_samples * 2  # 2배 생성해서 필터링\n",
    "    \n",
    "    # 생성된 이미지\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    with torch.no_grad():\n",
    "        # 기존: noise = torch.randn(num_generate, z_dim, device=device)\n",
    "        # 수정: DCGAN 형식으로 (batch, z_dim, 1, 1)\n",
    "        noise = torch.randn(num_generate, z_dim, 1, 1, device=device)\n",
    "        \n",
    "        generated_images = generator(noise)\n",
    "        \n",
    "        # Discriminator로 품질 평가\n",
    "        d_scores = discriminator(generated_images.to(device))\n",
    "        \n",
    "        # CPU로 이동하고 정규화\n",
    "        generated_images = generated_images.cpu()\n",
    "        generated_images = torch.clamp(generated_images, -1, 1)\n",
    "        generated_images = (generated_images + 1) / 2\n",
    "        \n",
    "        # 필터링 (threshold 이상만 선택)\n",
    "        d_scores = d_scores.cpu().squeeze()\n",
    "        high_quality_mask = d_scores > threshold\n",
    "        \n",
    "        if high_quality_mask.sum() > 0:\n",
    "            filtered_images = generated_images[high_quality_mask]\n",
    "            filtered_scores = d_scores[high_quality_mask]\n",
    "            # 상위 num_samples개만 선택\n",
    "            if len(filtered_images) > num_samples:\n",
    "                top_indices = torch.topk(filtered_scores, num_samples)[1]\n",
    "                filtered_images = filtered_images[top_indices]\n",
    "        else:\n",
    "            # 필터링된 이미지가 없으면 상위 점수 이미지들 선택\n",
    "            top_indices = torch.topk(d_scores, num_samples)[1]\n",
    "            filtered_images = generated_images[top_indices]\n",
    "    \n",
    "    # 실제 이미지 정규화\n",
    "    if real_images.min() < 0:  # 이미 정규화된 경우\n",
    "        real_images = (real_images + 1) / 2\n",
    "    elif real_images.max() > 1:  # 0-255 범위\n",
    "        real_images = real_images / 255.0\n",
    "    \n",
    "    # 3행으로 비교 시각화\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 9))\n",
    "    \n",
    "    # 실제 이미지\n",
    "    real_grid = vutils.make_grid(real_images, nrow=8, normalize=False, padding=2)\n",
    "    real_grid_np = real_grid.permute(1, 2, 0).numpy()\n",
    "    ax1.imshow(real_grid_np, cmap='gray' if real_images.shape[1] == 1 else None)\n",
    "    ax1.set_title('Real Images (Forget Set)', fontsize=14)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # 모든 생성된 이미지 (처음 num_samples개)\n",
    "    all_gen_grid = vutils.make_grid(generated_images[:num_samples], nrow=8, normalize=False, padding=2)\n",
    "    all_gen_grid_np = all_gen_grid.permute(1, 2, 0).numpy()\n",
    "    ax2.imshow(all_gen_grid_np, cmap='gray' if generated_images.shape[1] == 1 else None)\n",
    "    ax2.set_title('All Generated Images', fontsize=14)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # 필터링된 고품질 이미지\n",
    "    filtered_grid = vutils.make_grid(filtered_images, nrow=8, normalize=False, padding=2)\n",
    "    filtered_grid_np = filtered_grid.permute(1, 2, 0).numpy()\n",
    "    ax3.imshow(filtered_grid_np, cmap='gray' if filtered_images.shape[1] == 1 else None)\n",
    "    ax3.set_title(f'High-Quality Filtered Images (D-score > {threshold})', fontsize=14)\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./real_vs_generated_filtered.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Discriminator 점수 통계 출력\n",
    "    print(f\"Discriminator Scores - Mean: {d_scores.mean():.3f}, Std: {d_scores.std():.3f}\")\n",
    "    print(f\"High-quality images (>{threshold}): {high_quality_mask.sum().item()}/{len(d_scores)}\")\n",
    "    print(\"Comparison image saved to: ./real_vs_generated_filtered.png\")\n",
    "\n",
    "\n",
    "# visualize_discriminator_scores 함수도 동일하게 수정\n",
    "def visualize_discriminator_scores(generator, discriminator, z_dim=100, device='cpu', num_samples=100):\n",
    "    \"\"\"\n",
    "    Discriminator 점수 분포를 히스토그램으로 시각화\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # ========== 여기도 수정! ==========\n",
    "        # 기존: noise = torch.randn(num_samples, z_dim, device=device)\n",
    "        # 수정: DCGAN 형식으로\n",
    "        noise = torch.randn(num_samples, z_dim, 1, 1, device=device)\n",
    "        \n",
    "        generated_images = generator(noise)\n",
    "        d_scores = discriminator(generated_images).cpu().squeeze().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(d_scores, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "    plt.xlabel('Discriminator Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Discriminator Scores for Generated Images')\n",
    "    plt.axvline(x=0.5, color='red', linestyle='--', label='Threshold (0.5)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('./discriminator_scores_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Discriminator scores - Min: {d_scores.min():.3f}, Max: {d_scores.max():.3f}\")\n",
    "    print(f\"Mean: {d_scores.mean():.3f}, Std: {d_scores.std():.3f}\")\n",
    "    print(\"Discriminator scores distribution saved to: ./discriminator_scores_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30ef2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_dataset_to_device(dataset, device):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for x, y in dataset:\n",
    "        images.append(x.to(device))\n",
    "        labels.append(torch.tensor(y).to(device))\n",
    "    return TensorDataset(torch.stack(images), torch.stack(labels))\n",
    "\n",
    "\n",
    "def add_backdoor_trigger(x):\n",
    "    x_bd = x.clone()\n",
    "    x_bd[:, 25:28, 25:28] = 0.9\n",
    "    return x_bd\n",
    "\n",
    "def evaluate_backdoor_asr(model, dataset, target_label, device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            x, y = dataset[i]\n",
    "            # 백도어 트리거 삽입\n",
    "            x_bd = add_backdoor_trigger(x).to(device)\n",
    "            x_bd = x_bd.unsqueeze(0)  # 배치 차원 추가\n",
    "\n",
    "            output = model(x_bd)\n",
    "            pred = output.argmax(dim=1).item()\n",
    "\n",
    "            total += 1\n",
    "            if pred == target_label:\n",
    "                correct += 1\n",
    "\n",
    "    asr = correct / total\n",
    "    return asr\n",
    "\n",
    "def select_model(args, train_dataset):\n",
    "    if args.model == 'cnn':\n",
    "        if args.dataset == 'cifar':\n",
    "            return CNNCifar(args=args)  # CIFAR-10용 CNN 추가 필요\n",
    "        else:\n",
    "            return CNNMnist(args=args)  # MNIST용 CNN\n",
    "    elif args.model == 'resnet':\n",
    "        return ResNet18(num_classes=args.num_classes)  # CIFAR-10용 ResNet\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3d44c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    args = args_parser()\n",
    "    \n",
    "    # ===================== 0. 랜덤 시드 설정 (재현성) =====================\n",
    "    seed = 42\n",
    "    import random\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    print(f\"Random seed set to: {seed}\")\n",
    "    \n",
    "    device = 'cuda' if args.gpu and torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    exp_details(args)\n",
    "\n",
    "    # ===================== 1. 데이터셋 로딩 및 초기화 =====================\n",
    "    train_dataset, test_dataset, unseen_dataset, user_groups = get_dataset(args)\n",
    "    full_dataset = train_dataset  \n",
    "\n",
    "    global_model = select_model(args, full_dataset).to(device)\n",
    "    global_model.train()\n",
    "\n",
    "    if args.dataset == 'cifar':\n",
    "        generator = Generator(z_dim=args.z_dim, img_shape=(3, 32, 32)).to(device)      \n",
    "        discriminator = Discriminator(img_shape=(3, 32, 32)).to(device)                \n",
    "    else:\n",
    "        generator = Generator(z_dim=args.z_dim).to(device)\n",
    "        discriminator = Discriminator().to(device)\n",
    "\n",
    "    # DCGAN 가중치 초기화 적용\n",
    "    from models import weights_init\n",
    "    generator.apply(weights_init)\n",
    "    discriminator.apply(weights_init)\n",
    "    \n",
    "    global_weights = global_model.state_dict()\n",
    "    train_loss, train_accuracy = [], []\n",
    "\n",
    "    forget_client = 0\n",
    "    forget_idxs = user_groups[forget_client]\n",
    "    retain_idxs = [i for i in range(len(train_dataset)) if i not in forget_idxs]\n",
    "    test_idxs = np.random.choice(len(test_dataset), len(forget_idxs), replace=False)\n",
    "\n",
    "    # Source-Free를 위한 IID Unseen 데이터 준비\n",
    "    from utils import create_iid_unseen_data\n",
    "    iid_unseen_dataset = create_iid_unseen_data(unseen_dataset, forget_idxs, train_dataset)\n",
    "    \n",
    "    # 데이터 분배 정보 출력 \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"DATASET DISTRIBUTION\".center(70))\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Total Training Data: {len(full_dataset):,}\")\n",
    "    print(f\"Test Data: {len(test_dataset):,}\")\n",
    "    print(f\"Unseen Data Pool: {len(unseen_dataset):,}\")\n",
    "    print(f\"\\nFederated Learning Setup:\")\n",
    "    print(f\"   Forget Set Size: {len(forget_idxs):,}\")\n",
    "    print(f\"   Retain Set Size: {len(retain_idxs):,}\")\n",
    "    print(f\"   IID Unseen Data for Generation: {len(iid_unseen_dataset):,}\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    # ===================== 2. Original 연합학습 (FedAvg 방식) =====================\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"PHASE 1: Original FEDERATED LEARNING\".center(70))\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    fedavg_start_time = time.time()\n",
    "    unlearning_request_epoch = args.epochs // 2\n",
    "    unlearning_requested = False\n",
    "\n",
    "    for epoch in tqdm(range(args.epochs), desc='Original FL Training'):\n",
    "        if epoch == unlearning_request_epoch and not unlearning_requested:\n",
    "            print(f\"\\n[UNLEARNING REQUEST] Client {forget_client} requests unlearning at epoch {epoch + 1}\")\n",
    "            unlearning_requested = True\n",
    "            # 언러닝 요청 시점의 모델 저장\n",
    "            unlearning_request_model = copy.deepcopy(global_model)\n",
    "            break  # 언러닝 요청시 기존 FL 중단\n",
    "\n",
    "        local_weights, local_losses = [], []\n",
    "        m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "        for idx in idxs_users:\n",
    "            local_model = LocalUpdate(args=args, dataset=full_dataset, idxs=user_groups[idx])\n",
    "            w, loss = local_model.update_weights(model=copy.deepcopy(global_model), global_round=epoch)\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(loss)\n",
    "\n",
    "        global_weights = average_weights(local_weights)\n",
    "        global_model.load_state_dict(global_weights)\n",
    "        \n",
    "        loss_avg = sum(local_losses) / len(local_losses)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            acc, _ = test_inference(args, global_model, test_dataset)\n",
    "            print(f\"[Standard FL] Epoch {epoch+1}/{args.epochs} | Loss: {loss_avg:.4f} | Acc: {acc*100:.2f}%\")\n",
    "\n",
    "    fedavg_time = time.time() - fedavg_start_time\n",
    "    \n",
    "    # 언러닝 요청이 없었다면 전체 학습 완료 모델 사용\n",
    "    if not unlearning_requested:\n",
    "        unlearning_request_model = copy.deepcopy(global_model)\n",
    "\n",
    "    # ===================== 3. Source-Free 합성 데이터 생성 =====================\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"PHASE 2: SOURCE-FREE SYNTHETIC DATA GENERATION\".center(70))\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    generation_start_time = time.time()\n",
    "    \n",
    "    print(\"[Source-Free] Server provides IID unseen data to unlearning client...\")\n",
    "    print(\"[Source-Free] Client generates synthetic data without direct forget data access...\")\n",
    "    \n",
    "    # FedEraser와 결합한 Source-Free 생성: Forget 분포 + Unseen 스타일\n",
    "    generator, discriminator = train_gd_ungan_with_unseen(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        dataset=full_dataset,\n",
    "        retain_idxs=retain_idxs,\n",
    "        forget_idxs=forget_idxs,\n",
    "        device=device,\n",
    "        lambda_adv=1.0,\n",
    "        z_dim=args.z_dim,\n",
    "        batch_size=64,\n",
    "        epochs=200,  \n",
    "        unseen_dataset=iid_unseen_dataset,\n",
    "        mixing_ratio=0.5  \n",
    "    )\n",
    "\n",
    "    # 합성 데이터 생성\n",
    "    synthetic_images, synthetic_labels = generate_fixed_threshold_data(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        forget_idxs=forget_idxs,\n",
    "        dataset=full_dataset,\n",
    "        device=device,\n",
    "        z_dim=args.z_dim,\n",
    "        target_count=len(forget_idxs),\n",
    "        fixed_threshold=args.gen_threshold,\n",
    "        batch_size=64\n",
    "    )\n",
    "\n",
    "    synthetic_dataset = SyntheticImageDataset(synthetic_images, synthetic_labels)\n",
    "    generation_time = time.time() - generation_start_time\n",
    "    \n",
    "    print(f\"[Source-Free] Generated {len(synthetic_images):,} synthetic samples in {generation_time:.2f}s\")\n",
    "\n",
    "    # ===================== 4. 단순화된 FedEraser 언러닝 =====================\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"PHASE 3: SIMPLIFIED FEDERASER UNLEARNING PROCESS\".center(70))\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    unlearning_start_time = time.time()\n",
    "    \n",
    "    # 4-1. 언러닝 직전 상태에서 언러닝 클라이언트 제외한 가중치 저장\n",
    "    print(\"[FedEraser] Saving pre-unlearning weights excluding unlearning client...\")\n",
    "    \n",
    "    # 언러닝 클라이언트 제외한 나머지 클라이언트들의 평균 가중치 계산\n",
    "    other_clients = list(range(args.num_users))\n",
    "    other_clients.remove(forget_client)\n",
    "    \n",
    "    # 언러닝 요청 직전 상태에서 나머지 클라이언트들만의 가중치 근사 계산\n",
    "    # 실제로는 언러닝 요청 시점의 글로벌 모델에서 언러닝 클라이언트 기여도를 제외\n",
    "    exclude_unlearn_weights = copy.deepcopy(unlearning_request_model.state_dict())\n",
    "    \n",
    "    # 4-2. 언러닝 클라이언트: 합성 데이터로 로컬 학습\n",
    "    print(\"[FedEraser] Unlearning client training with synthetic data...\")\n",
    "    unlearn_local = LocalUpdate(args=args, dataset=synthetic_dataset)\n",
    "    unlearn_weights, unlearn_loss, unlearn_delta = unlearn_local.synthetic_update_weights(\n",
    "        model=copy.deepcopy(unlearning_request_model), \n",
    "        global_round=0,\n",
    "        synthetic_dataset=synthetic_dataset\n",
    "    )\n",
    "    \n",
    "    print(f\"[FedEraser] Unlearning client completed local training with loss: {unlearn_loss:.4f}\")\n",
    "    \n",
    "    # 4-3. 가중치 조합 (단순화된 FedEraser)\n",
    "    print(\"[FedEraser] Combining weights...\")\n",
    "    \n",
    "    # 단순한 가중 평균: 기존 (언러닝 클라이언트 제외) + 새로운 언러닝 가중치\n",
    "    num_other_clients = len(other_clients)\n",
    "    total_clients = num_other_clients + 1  # 언러닝 클라이언트 포함\n",
    "    \n",
    "    combined_weights = {}\n",
    "    for key in exclude_unlearn_weights.keys():\n",
    "        # 가중 평균: (나머지 클라이언트 가중치 * 수) + (언러닝 클라이언트 가중치 * 1) / 전체\n",
    "        combined_weights[key] = (\n",
    "            exclude_unlearn_weights[key] * num_other_clients + \n",
    "            unlearn_weights[key] * 1\n",
    "        ) / total_clients\n",
    "    \n",
    "    # 4-4. 언러닝된 모델 생성\n",
    "    finetune_model = copy.deepcopy(unlearning_request_model)\n",
    "    finetune_model.load_state_dict(combined_weights)\n",
    "    \n",
    "    print(\"[FedEraser] Weight combination completed\")\n",
    "    \n",
    "    # 4-5. 언러닝 클라이언트 제외하고 연합학습 재개\n",
    "    print(\"[FedEraser] Resuming federated learning without unlearning client...\")\n",
    "    \n",
    "    remaining_epochs = args.epochs - unlearning_request_epoch - 1\n",
    "    for epoch in tqdm(range(remaining_epochs), desc='Post-Unlearning FL'):\n",
    "        local_weights, local_losses = [], []\n",
    "        \n",
    "        # 언러닝 클라이언트 제외한 연합학습\n",
    "        m = max(int(args.frac * len(other_clients)), 1)\n",
    "        idxs_users = np.random.choice(other_clients, m, replace=False)\n",
    "\n",
    "        for idx in idxs_users:\n",
    "            local_model = LocalUpdate(args=args, dataset=full_dataset, idxs=user_groups[idx])\n",
    "            w, loss = local_model.update_weights(model=copy.deepcopy(finetune_model), global_round=epoch)\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(loss)\n",
    "\n",
    "        if local_weights:\n",
    "            global_weights = average_weights(local_weights)\n",
    "            finetune_model.load_state_dict(global_weights)\n",
    "            \n",
    "            loss_avg = sum(local_losses) / len(local_losses)\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"[Post-Unlearning FL] Epoch {epoch+1}/{remaining_epochs} | Loss: {loss_avg:.4f}\")\n",
    "\n",
    "    unlearning_time = time.time() - unlearning_start_time\n",
    "\n",
    "    # ===================== 5. Original 모델 (언러닝 요청 무시) =====================\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"BASELINE: ORIGINAL MODEL (IGNORING UNLEARNING)\".center(70))\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    original_continue_start = time.time()\n",
    "    original_model = copy.deepcopy(unlearning_request_model)\n",
    "    \n",
    "    # 언러닝 요청을 무시하고 모든 클라이언트와 계속 학습\n",
    "    remaining_epochs = args.epochs - unlearning_request_epoch - 1\n",
    "    for epoch in tqdm(range(remaining_epochs), desc='Original Continue Training'):\n",
    "        local_weights, local_losses = [], []\n",
    "        m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "        for idx in idxs_users:\n",
    "            local_model = LocalUpdate(args=args, dataset=full_dataset, idxs=user_groups[idx])\n",
    "            w, loss = local_model.update_weights(model=copy.deepcopy(original_model), global_round=epoch)\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(loss)\n",
    "\n",
    "        global_weights = average_weights(local_weights)\n",
    "        original_model.load_state_dict(global_weights)\n",
    "\n",
    "    original_continue_time = time.time() - original_continue_start\n",
    "\n",
    "    # ===================== 6. Retrain 모델 (이상적인 기준점) =====================\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"BASELINE: RETRAIN MODEL (IDEAL UNLEARNING)\".center(70))\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    retrain_start_time = time.time()\n",
    "    retrain_model = select_model(args, full_dataset).to(device)\n",
    "    \n",
    "    # 언러닝 클라이언트 제외하고 처음부터 재훈련\n",
    "    for epoch in tqdm(range(args.epochs), desc='Retrain FL Training'):\n",
    "        local_weights, local_losses = [], []\n",
    "        m = max(int(args.frac * len(other_clients)), 1)\n",
    "        idxs_users = np.random.choice(other_clients, m, replace=False)\n",
    "\n",
    "        for idx in idxs_users:\n",
    "            local_model = LocalUpdate(args=args, dataset=full_dataset, idxs=user_groups[idx])\n",
    "            w, loss = local_model.update_weights(model=copy.deepcopy(retrain_model), global_round=epoch)\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(loss)\n",
    "\n",
    "        if local_weights:\n",
    "            global_weights = average_weights(local_weights)\n",
    "            retrain_model.load_state_dict(global_weights)\n",
    "\n",
    "    retrain_time = time.time() - retrain_start_time\n",
    "\n",
    "    # ===================== 7. 종합 평가 =====================\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"COMPREHENSIVE EVALUATION\".center(70))\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    from evaluate_mia import evaluate_model_comparison\n",
    "    \n",
    "    evaluation_results = evaluate_model_comparison(\n",
    "        original_model=original_model,\n",
    "        retrain_model=retrain_model,\n",
    "        finetune_model=finetune_model,\n",
    "        train_dataset=full_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        forget_idxs=forget_idxs,\n",
    "        retain_idxs=retain_idxs,\n",
    "        synthetic_dataset=synthetic_dataset,\n",
    "        device=device,\n",
    "        save_path='./results/comprehensive_evaluation.json'\n",
    "    )\n",
    "\n",
    "    # ===================== 8. IID vs Non-IID 비교 (필요시) =====================\n",
    "    if not args.iid:\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"IID vs NON-IID COMPARISON\".center(70))\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        # IID 설정으로 한 번 더 실험\n",
    "        args_iid = copy.deepcopy(args)\n",
    "        args_iid.iid = 1\n",
    "        \n",
    "        train_iid, test_iid, _, groups_iid = get_dataset(args_iid)\n",
    "        \n",
    "        datasets_dict = {\n",
    "            'noniid': {\n",
    "                'train_dataset': full_dataset,\n",
    "                'test_dataset': test_dataset,\n",
    "                'forget_idxs': forget_idxs,\n",
    "                'retain_idxs': retain_idxs\n",
    "            },\n",
    "            'iid': {\n",
    "                'train_dataset': train_iid,\n",
    "                'test_dataset': test_iid,\n",
    "                'forget_idxs': groups_iid[forget_client],\n",
    "                'retain_idxs': [idx for i, group in groups_iid.items() \n",
    "                               if i != forget_client for idx in group]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        models_dict = {\n",
    "            'Original': original_model,\n",
    "            'Retrain': retrain_model,\n",
    "            'Finetune': finetune_model\n",
    "        }\n",
    "        \n",
    "        from evaluate_mia import evaluate_iid_vs_noniid\n",
    "        iid_comparison = evaluate_iid_vs_noniid(\n",
    "            models_dict=models_dict,\n",
    "            datasets_dict=datasets_dict,\n",
    "            device=device,\n",
    "            save_path='./results/iid_vs_noniid_comparison.json'\n",
    "        )\n",
    "\n",
    "    # ===================== 9. 최종 결과 요약 =====================\n",
    "    end_time = time.time()\n",
    "    total_experiment_time = end_time - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"FINAL EXPERIMENT SUMMARY\".center(70))\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    print(f\"\\nDataset Summary:\")\n",
    "    print(f\"   Total Training Data: {len(full_dataset):,}\")\n",
    "    print(f\"   Forget Set Size: {len(forget_idxs):,}\")\n",
    "    print(f\"   Retain Set Size: {len(retain_idxs):,}\")\n",
    "    print(f\"   Test Set Size: {len(test_dataset):,}\")\n",
    "    print(f\"   Generated Synthetic Data: {len(synthetic_images):,}\")\n",
    "    \n",
    "    print(f\"\\nApproach Summary:\")\n",
    "    print(f\"   Source-Free: ✓ (No direct forget data access)\")\n",
    "    print(f\"   FedEraser Integration: ✓ (Delta weight combination)\")\n",
    "    print(f\"   IID Unseen Data: ✓ (Matching forget distribution)\")\n",
    "    print(f\"   Synthetic Generation: ✓ (GAN-based data synthesis)\")\n",
    "    \n",
    "    print(f\"\\nTiming Analysis:\")\n",
    "    print(f\"   Standard FL:       {fedavg_time:8.2f}s\")\n",
    "    print(f\"   Unlearning Process: {unlearning_time:8.2f}s\")\n",
    "    print(f\"     └── Generation:  {generation_time:8.2f}s\")\n",
    "    print(f\"   Retrain: {retrain_time:8.2f}s\")\n",
    "    print(f\"   Total Experiment:   {total_experiment_time:8.2f}s\")\n",
    "    \n",
    "    print(f\"\\nPerformance Comparison:\")\n",
    "    for model_name, results in evaluation_results.items():\n",
    "        print(f\"   {model_name:10} | Test: {results['test_accuracy']:.3f} | \"\n",
    "              f\"Retain: {results['retain_accuracy']:.3f} | \"\n",
    "              f\"Forget: {results['forget_accuracy']:.3f} | \"\n",
    "              f\"MIA: {results['mia_auc']:.3f}\")\n",
    "\n",
    "\n",
    "    # ===================== 10. MIA 평가 (기존 형식 유지) =====================\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"MEMBERSHIP INFERENCE ATTACK EVALUATION\".center(70))\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    all_idxs = set(range(len(full_dataset)))\n",
    "    non_member_candidates = list(all_idxs - set(forget_idxs))\n",
    "    \n",
    "    # Original Model MIA 평가\n",
    "    print(f\"\\n[MIA] Original Model (Before Unlearning):\")\n",
    "    mia_result_before = evaluate_mia(\n",
    "        model=original_model,\n",
    "        dataset=full_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        forget_idxs=forget_idxs,\n",
    "        retain_idxs=test_idxs,\n",
    "        shadow_idxs=np.random.choice(non_member_candidates, len(forget_idxs), replace=False),\n",
    "        device=device,\n",
    "        save_path=\"./mia_result_before.json\"\n",
    "    )\n",
    "    \n",
    "    # Retrain Model MIA 평가\n",
    "    print(f\"\\n[MIA] Retrain Model (Gold Standard):\")\n",
    "    mia_retrain_after = evaluate_mia(\n",
    "        model=retrain_model,\n",
    "        dataset=full_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        forget_idxs=forget_idxs,\n",
    "        retain_idxs=test_idxs,\n",
    "        shadow_idxs=np.random.choice(non_member_candidates, len(forget_idxs), replace=False),\n",
    "        device=device,\n",
    "        save_path=\"./mia_result_retrain.json\"\n",
    "    )\n",
    "    \n",
    "    # Finetune Model MIA 평가\n",
    "    print(f\"\\n[MIA] Finetune Model (Our Method):\")\n",
    "    mia_finetune_after = evaluate_mia(\n",
    "        model=finetune_model,\n",
    "        dataset=full_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        forget_idxs=forget_idxs,\n",
    "        retain_idxs=test_idxs,\n",
    "        shadow_idxs=np.random.choice(non_member_candidates, len(forget_idxs), replace=False),\n",
    "        device=device,\n",
    "        save_path=\"./mia_result_finetune.json\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'original_acc': original_acc_final, 'retrain_acc': retrain_acc_final, 'finetune_acc': finetune_acc_final,\n",
    "        'original_time': original_total_time, 'retrain_time': retrain_time, 'unlearning_time': unlearning_time,\n",
    "        'generation_time': generation_time, 'finetune_training_time': unlearning_time - generation_time, \n",
    "        'synthetic_count': len(synthetic_images),\n",
    "        'mia_scores': {'original': mia_result_before['auc'], 'retrain': mia_retrain_after['auc'], 'finetune': mia_finetune_after['auc']}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404126b",
   "metadata": {},
   "source": [
    "# 1. Original Model\n",
    "\n",
    "모든 에포크, 모든 클라이언트와 학습. 언러닝 요청 무시\n",
    "\n",
    "# 2. Retrain Model \n",
    "\n",
    "완전히 새로운 모델로 시작. 처음부터 전체 에포크 동안 forget_client 없이 학습. 이상적인 완전 언러닝의 결과\n",
    "\n",
    "# 3. Finetune Model (Our Method)\n",
    "\n",
    "언러닝 요청 시점의 모델부터 시작. 나머지 에포크 동안 합성 데이터로 복구. 실용적인 언러닝 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3498a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.argv = [\n",
    "    'ipykernel_launcher.py',\n",
    "    '--epochs', '25',\n",
    "    '--num_users', '10',\n",
    "    '--frac', '1.0',\n",
    "    '--local_ep', '10',\n",
    "    '--local_bs', '64',\n",
    "    '--lr', '0.01',\n",
    "    '--momentum', '0.9',\n",
    "    '--dataset', 'cifar',\n",
    "    '--model', 'resnet',\n",
    "    '--iid', '0',\n",
    "    '--gpu', '0',\n",
    "    '--num_classes', '10',\n",
    "    '--dirichlet', '1',\n",
    "    '--alpha', '0.3',\n",
    "    '--load_model', 'None',\n",
    "    '--save_model', './saved_models/model.pth',\n",
    "    '--z_dim', '100',\n",
    "    '--gen_threshold', '0.2',\n",
    "    '--num_gen_samples', '128',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f8646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 42\n",
      "\n",
      "======================================================================\n",
      "                       EXPERIMENT SETTINGS\n",
      "======================================================================\n",
      "Model           : resnet\n",
      "Dataset         : cifar\n",
      "Num Clients     : 10\n",
      "Fraction        : 1.0\n",
      "IID             : 0\n",
      "dirichlet alpha : 0.3\n",
      "Epoch           : 25\n",
      "Local Epochs    : 10\n",
      "Batch Size      : 64\n",
      "Learning Rate   : 0.01\n",
      "Generator z_dim : 100\n",
      "Disc. Threshold : 0.2\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "                         DATASET DISTRIBUTION                         \n",
      "----------------------------------------------------------------------\n",
      "Total Training Data: 45,000\n",
      "Test Data: 10,000\n",
      "Unseen Data Pool: 5,000\n",
      "\n",
      "Federated Learning Setup:\n",
      "   Forget Set Size: 2,339\n",
      "   Retain Set Size: 42,661\n",
      "   IID Unseen Data for Generation: 2,339\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "                 PHASE 1: Original FEDERATED LEARNING                 \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Original FL Training:  20%|██        | 5/25 [18:53<1:16:24, 229.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Standard FL] Epoch 5/25 | Loss: 0.0953 | Acc: 58.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Original FL Training:  24%|██▍       | 6/25 [22:50<1:13:28, 232.05s/it]"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

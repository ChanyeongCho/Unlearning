{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daef1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset, DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference\n",
    "from models import (CNNCifar, CNNMnist, ResNet18, \n",
    "                   GeneratorCifar, DiscriminatorCifar, Generator, Discriminator,\n",
    "                   generate_images, filter_images, add_backdoor_trigger_cifar, add_backdoor_trigger_mnist)\n",
    "from utils import get_dataset, average_weights, exp_details, create_poisoned_dataset\n",
    "from unlearn import (\n",
    "    train_generator_ungan,\n",
    "    SyntheticImageDataset,\n",
    "    partition_synthetic_data_iid,\n",
    "    get_synthetic_subset\n",
    ")\n",
    "from evaluate_mia import evaluate_mia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2fb9d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset, DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference\n",
    "from models import (CNNCifar, CNNMnist, ResNet18, \n",
    "                   GeneratorCifar, DiscriminatorCifar, Generator, Discriminator,\n",
    "                   generate_images, filter_images, add_backdoor_trigger_cifar, add_backdoor_trigger_mnist)\n",
    "from utils import get_dataset, average_weights, exp_details, create_poisoned_dataset\n",
    "from unlearn import (\n",
    "    train_generator_ungan,\n",
    "    SyntheticImageDataset,\n",
    "    partition_synthetic_data_iid,\n",
    "    get_synthetic_subset\n",
    ")\n",
    "from evaluate_mia import evaluate_mia\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "# =================== Image Visualization Functions ===================\n",
    "\n",
    "def denormalize_image(tensor, dataset_type='cifar'):\n",
    "    \"\"\"ë°ì´í„°ì…‹ íƒ€ì…ì— ë”°ë¥¸ ì •ê·œí™” í•´ì œ\"\"\"\n",
    "    if dataset_type == 'cifar':\n",
    "        mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "    else:  # mnist\n",
    "        mean = torch.tensor([0.1307]).view(1, 1, 1)\n",
    "        std = torch.tensor([0.3081]).view(1, 1, 1)\n",
    "    \n",
    "    denormalized = tensor * std + mean\n",
    "    return torch.clamp(denormalized, 0, 1)\n",
    "\n",
    "\n",
    "def visualize_hybrid_generation(generator, discriminator, dataset, unseen_dataset, \n",
    "                               forget_idxs, device, z_dim=100, num_samples=16, dataset_type='cifar'):\n",
    "    \"\"\"Hybrid ì´ë¯¸ì§€ ìƒì„± ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì‹œê°í™”\"\"\"\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    # 1. ì›ë³¸ Forget ì´ë¯¸ì§€ë“¤\n",
    "    forget_samples = []\n",
    "    forget_labels = []\n",
    "    for i in range(min(num_samples//4, len(forget_idxs))):\n",
    "        img, label = dataset[forget_idxs[i]]\n",
    "        forget_samples.append(img)\n",
    "        forget_labels.append(label)\n",
    "    \n",
    "    # 2. Unseen ì´ë¯¸ì§€ë“¤\n",
    "    unseen_samples = []\n",
    "    for i in range(min(num_samples//4, len(unseen_dataset))):\n",
    "        img, _ = unseen_dataset[i]\n",
    "        unseen_samples.append(img)\n",
    "    \n",
    "    # 3. ê¸°ë³¸ ìƒì„± ì´ë¯¸ì§€\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn((num_samples//4, z_dim), device=device)\n",
    "        basic_generated = generator(noise).cpu()\n",
    "    \n",
    "    # 4. Hybrid ì´ë¯¸ì§€\n",
    "    unseen_style = torch.stack(unseen_samples).mean(dim=0, keepdim=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn((num_samples//4, z_dim), device=device)\n",
    "        gen_imgs = generator(noise)\n",
    "        hybrid_generated = style_blend(gen_imgs, unseen_style, strength=0.3).cpu()\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    fig, axes = plt.subplots(4, num_samples//4, figsize=(16, 12))\n",
    "    fig.suptitle(f'Advanced UNGAN: Hybrid Image Generation Process ({dataset_type.upper()})', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    stages = [\n",
    "        (forget_samples, \"1. Original Forget Data\", 'Reds'),\n",
    "        (unseen_samples, \"2. Unseen Style Reference\", 'Blues'), \n",
    "        (basic_generated, \"3. Basic Generated\", 'Greens'),\n",
    "        (hybrid_generated, \"4. Hybrid (Forget+Unseen)\", 'Purples')\n",
    "    ]\n",
    "    \n",
    "    for row, (images, title, cmap) in enumerate(stages):\n",
    "        for col in range(len(images)):\n",
    "            ax = axes[row, col]\n",
    "            img = denormalize_image(images[col], dataset_type)\n",
    "            \n",
    "            # MNISTëŠ” í‘ë°±, CIFARëŠ” ì»¬ëŸ¬\n",
    "            if dataset_type == 'mnist':\n",
    "                img_np = img.squeeze().numpy()  # (28, 28)\n",
    "                ax.imshow(img_np, cmap='gray')\n",
    "            else:\n",
    "                img_np = img.permute(1, 2, 0).numpy()  # (32, 32, 3)\n",
    "                ax.imshow(img_np)\n",
    "            \n",
    "            ax.axis('off')\n",
    "            \n",
    "            if col == 0:\n",
    "                ax.text(-0.1, 0.5, title, rotation=90, verticalalignment='center',\n",
    "                       fontsize=12, fontweight='bold', transform=ax.transAxes)\n",
    "            \n",
    "            if row == 0:\n",
    "                ax.set_title(f'Class: {forget_labels[col]}', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./hybrid_generation_process_{dataset_type}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return hybrid_generated\n",
    "\n",
    "\n",
    "def save_individual_images(hybrid_images, save_dir='./generated_samples', dataset_type='cifar'):\n",
    "    \"\"\"ê°œë³„ hybrid ì´ë¯¸ì§€ë“¤ì„ íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    for i, img in enumerate(hybrid_images):\n",
    "        img_denorm = denormalize_image(img, dataset_type)\n",
    "        \n",
    "        if dataset_type == 'mnist':\n",
    "            # MNIST: í‘ë°± ì´ë¯¸ì§€ ì²˜ë¦¬\n",
    "            img_denorm = img_denorm.squeeze()  # (1, 28, 28) -> (28, 28)\n",
    "        \n",
    "        img_pil = transforms.ToPILImage()(img_denorm)\n",
    "        img_pil.save(os.path.join(save_dir, f'hybrid_sample_{dataset_type}_{i:03d}.png'))\n",
    "    \n",
    "    print(f\"ğŸ’¾ Saved {len(hybrid_images)} {dataset_type.upper()} hybrid images to {save_dir}/\")\n",
    "\n",
    "\n",
    "def visualize_advanced_ungan_results(generator, discriminator, dataset, unseen_dataset,\n",
    "                                   forget_idxs, filtered_images, filtered_labels, \n",
    "                                   device, args, dataset_type='cifar'):\n",
    "    \"\"\"Advanced UNGAN ê²°ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ì‹œê°í™”\"\"\"\n",
    "    print(f\"\\nğŸ¨ ========== Visualizing Advanced UNGAN Results ({dataset_type.upper()}) ==========\")\n",
    "    \n",
    "    # 1. Hybrid ìƒì„± ê³¼ì • ì‹œê°í™”\n",
    "    print(\"ğŸ“¸ 1. Creating hybrid generation process visualization...\")\n",
    "    try:\n",
    "        hybrid_samples = visualize_hybrid_generation(\n",
    "            generator, discriminator, dataset, unseen_dataset, \n",
    "            forget_idxs, device, args.z_dim, num_samples=16, dataset_type=dataset_type\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Visualization error: {e}\")\n",
    "        hybrid_samples = filtered_images[:16] if len(filtered_images) >= 16 else filtered_images\n",
    "    \n",
    "    # 2. ê°œë³„ ì´ë¯¸ì§€ ì €ì¥\n",
    "    print(\"ğŸ’¾ 2. Saving individual hybrid images...\")\n",
    "    if len(filtered_images) > 0:\n",
    "        save_individual_images(filtered_images[:50], dataset_type=dataset_type)\n",
    "        \n",
    "        # ë¹„êµìš© ì›ë³¸ ì´ë¯¸ì§€ë„ ì €ì¥\n",
    "        os.makedirs('./original_samples', exist_ok=True)\n",
    "        for i, idx in enumerate(forget_idxs[:20]):\n",
    "            img, label = dataset[idx]\n",
    "            img_denorm = denormalize_image(img, dataset_type)\n",
    "            \n",
    "            if dataset_type == 'mnist':\n",
    "                img_denorm = img_denorm.squeeze()\n",
    "            \n",
    "            img_pil = transforms.ToPILImage()(img_denorm)\n",
    "            img_pil.save(f'./original_samples/forget_sample_{dataset_type}_{i:03d}_class{label}.png')\n",
    "        \n",
    "        print(f\"ğŸ’¾ Also saved original {dataset_type.upper()} forget samples to ./original_samples/\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No hybrid images to save\")\n",
    "    \n",
    "    print(\"âœ… Visualization completed! Check these files:\")\n",
    "    print(f\"   ğŸ“Š hybrid_generation_process_{dataset_type}.png - 4ë‹¨ê³„ ìƒì„± ê³¼ì •\")\n",
    "    print(\"   ğŸ–¼ï¸  ./generated_samples/ - Hybrid ì´ë¯¸ì§€ë“¤\")  \n",
    "    print(\"   ğŸ“‚ ./original_samples/ - ì›ë³¸ Forget ì´ë¯¸ì§€ë“¤\")\n",
    "    \n",
    "    return hybrid_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495d7781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== Advanced UNGAN Functions ===================\n",
    "\n",
    "def extract_intermediate_features(discriminator, images):\n",
    "    \"\"\"Discriminatorì˜ ì¤‘ê°„ ë ˆì´ì–´ì—ì„œ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ\"\"\"\n",
    "    features = images\n",
    "    for i, layer in enumerate(discriminator.model):\n",
    "        features = layer(features)\n",
    "        if i == 4:  # ì¤‘ê°„ ë ˆì´ì–´ì—ì„œ íŠ¹ì§• ì¶”ì¶œ\n",
    "            break\n",
    "    return features.view(features.size(0), -1)\n",
    "\n",
    "\n",
    "def style_blend(generated_imgs, style_reference, strength=0.3):\n",
    "    \"\"\"ìƒì„±ëœ ì´ë¯¸ì§€ì— ì°¸ì¡° ìŠ¤íƒ€ì¼ ë¸”ë Œë”©\"\"\"\n",
    "    # ìƒ‰ì¡° ë° ë°ê¸° ì¡°ì •\n",
    "    gen_mean = generated_imgs.mean(dim=[2, 3], keepdim=True)\n",
    "    gen_std = generated_imgs.std(dim=[2, 3], keepdim=True)\n",
    "    \n",
    "    style_mean = style_reference.mean(dim=[2, 3], keepdim=True)\n",
    "    style_std = style_reference.std(dim=[2, 3], keepdim=True)\n",
    "    \n",
    "    # ìŠ¤íƒ€ì¼ ì „ì´ (AdaIN ë°©ì‹ ê°„ì†Œí™”)\n",
    "    normalized = (generated_imgs - gen_mean) / (gen_std + 1e-8)\n",
    "    stylized = normalized * (gen_std * (1-strength) + style_std * strength) + \\\n",
    "               (gen_mean * (1-strength) + style_mean * strength)\n",
    "    \n",
    "    return torch.clamp(stylized, -1, 1)\n",
    "\n",
    "\n",
    "def train_hybrid_generator(generator, discriminator, dataset, unseen_dataset, \n",
    "                          retain_idxs, forget_idxs, device,\n",
    "                          alpha=0.7, beta=0.3, z_dim=100, batch_size=64, epochs=50):\n",
    "    \"\"\"\n",
    "    Forget íŠ¹ì„± + Unseen íŠ¹ì„±ì„ ìœµí•©í•˜ëŠ” ê³ ê¸‰ Generator í›ˆë ¨\n",
    "    \"\"\"\n",
    "    print(f\"[Hybrid UNGAN] Training with Î±={alpha} (forget) + Î²={beta} (unseen)\")\n",
    "    \n",
    "    g_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)\n",
    "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "    \n",
    "    # ë°ì´í„° ë¡œë” ì¤€ë¹„\n",
    "    forget_subset = Subset(dataset, forget_idxs)\n",
    "    forget_loader = DataLoader(forget_subset, batch_size=batch_size//2, shuffle=True, drop_last=True)\n",
    "    unseen_loader = DataLoader(unseen_dataset, batch_size=batch_size//2, shuffle=True, drop_last=True)\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_g_loss = 0\n",
    "        epoch_d_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # ë‘ ë°ì´í„° ë¡œë”ë¥¼ ë™ì‹œì— ìˆœíšŒ\n",
    "        forget_iter = iter(forget_loader)\n",
    "        unseen_iter = iter(unseen_loader)\n",
    "        \n",
    "        for _ in range(min(len(forget_loader), len(unseen_loader))):\n",
    "            try:\n",
    "                forget_batch, forget_labels = next(forget_iter)\n",
    "                unseen_batch, unseen_labels = next(unseen_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "                \n",
    "            forget_batch = forget_batch.to(device)\n",
    "            unseen_batch = unseen_batch.to(device)\n",
    "            \n",
    "            batch_size_actual = min(forget_batch.size(0), unseen_batch.size(0))\n",
    "            forget_batch = forget_batch[:batch_size_actual]\n",
    "            unseen_batch = unseen_batch[:batch_size_actual]\n",
    "            \n",
    "            # Discriminator í›ˆë ¨\n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            # ì‹¤ì œ ë°ì´í„° (forget=0, unseen=1)\n",
    "            real_forget_labels = torch.zeros((batch_size_actual, 1), device=device)\n",
    "            real_unseen_labels = torch.ones((batch_size_actual, 1), device=device)\n",
    "            fake_labels = torch.full((batch_size_actual, 1), 0.5, device=device)\n",
    "            \n",
    "            # Discriminator ì†ì‹¤ ê³„ì‚°\n",
    "            d_pred_forget = discriminator(forget_batch)\n",
    "            d_loss_forget = F.binary_cross_entropy(d_pred_forget, real_forget_labels)\n",
    "            \n",
    "            d_pred_unseen = discriminator(unseen_batch)\n",
    "            d_loss_unseen = F.binary_cross_entropy(d_pred_unseen, real_unseen_labels)\n",
    "            \n",
    "            z = torch.randn((batch_size_actual, z_dim), device=device)\n",
    "            fake_imgs = generator(z)\n",
    "            d_pred_fake = discriminator(fake_imgs.detach())\n",
    "            d_loss_fake = F.binary_cross_entropy(d_pred_fake, fake_labels)\n",
    "            \n",
    "            d_loss = d_loss_forget + d_loss_unseen + d_loss_fake\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            # Generator í›ˆë ¨\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            z = torch.randn((batch_size_actual, z_dim), device=device)\n",
    "            gen_imgs = generator(z)\n",
    "            \n",
    "            # Strategy A: Forget íŠ¹ì„± ìœ ì§€\n",
    "            d_pred_gen = discriminator(gen_imgs)\n",
    "            forget_target = torch.zeros((batch_size_actual, 1), device=device)\n",
    "            loss_forget_mimic = F.binary_cross_entropy(d_pred_gen, forget_target)\n",
    "            \n",
    "            # Strategy B: Unseen íŠ¹ì„± ëª¨ë°©\n",
    "            try:\n",
    "                unseen_features = extract_intermediate_features(discriminator, unseen_batch)\n",
    "                gen_features = extract_intermediate_features(discriminator, gen_imgs)\n",
    "                loss_unseen_style = F.mse_loss(gen_features, unseen_features)\n",
    "            except:\n",
    "                # íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ í”½ì…€ ë ˆë²¨ ìœ ì‚¬ì„± ì‚¬ìš©\n",
    "                loss_unseen_style = F.mse_loss(gen_imgs.mean(dim=[2,3]), unseen_batch.mean(dim=[2,3]))\n",
    "            \n",
    "            # ì´ Generator ì†ì‹¤\n",
    "            g_loss = alpha * loss_forget_mimic + beta * loss_unseen_style\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            epoch_g_loss += g_loss.item()\n",
    "            epoch_d_loss += d_loss.item()\n",
    "            batch_count += 1\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            avg_g_loss = epoch_g_loss / max(batch_count, 1)\n",
    "            avg_d_loss = epoch_d_loss / max(batch_count, 1)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | G Loss: {avg_g_loss:.4f} | D Loss: {avg_d_loss:.4f}\")\n",
    "    \n",
    "    print(\"[Hybrid UNGAN] Training completed!\")\n",
    "    return generator\n",
    "\n",
    "\n",
    "def generate_hybrid_images(generator, forget_idxs, unseen_dataset, dataset, \n",
    "                          device='cpu', z_dim=100, style_strength=0.3):\n",
    "    \"\"\"Forget ë‚´ìš© + Unseen ìŠ¤íƒ€ì¼ì˜ hybrid ì´ë¯¸ì§€ ìƒì„±\"\"\"\n",
    "    generator.eval()\n",
    "    device = torch.device(device)\n",
    "    num_samples = len(forget_idxs)\n",
    "    \n",
    "    # Forget ë°ì´í„°ì˜ ë ˆì´ë¸” ì •ë³´ ìœ ì§€\n",
    "    forget_labels = torch.tensor([dataset[i][1] for i in forget_idxs], dtype=torch.long)\n",
    "    \n",
    "    # Unseen ë°ì´í„°ì—ì„œ ìŠ¤íƒ€ì¼ ì°¸ì¡°\n",
    "    unseen_samples = torch.stack([unseen_dataset[i][0] for i in range(min(100, len(unseen_dataset)))])\n",
    "    unseen_style = unseen_samples.mean(dim=0, keepdim=True).to(device)\n",
    "    \n",
    "    generated_images = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_size = 32\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_end = min(i + batch_size, num_samples)\n",
    "            batch_size_actual = batch_end - i\n",
    "            \n",
    "            noise = torch.randn((batch_size_actual, z_dim), device=device)\n",
    "            gen_imgs = generator(noise)\n",
    "            \n",
    "            # Post-processing: Unseen ìŠ¤íƒ€ì¼ ë¸”ë Œë”©\n",
    "            if style_strength > 0:\n",
    "                gen_imgs = style_blend(gen_imgs, unseen_style, strength=style_strength)\n",
    "            \n",
    "            generated_images.append(gen_imgs.cpu())\n",
    "    \n",
    "    final_images = torch.cat(generated_images, dim=0)\n",
    "    return final_images, forget_labels\n",
    "\n",
    "\n",
    "def advanced_filter_images(discriminator, images, labels, \n",
    "                          forget_threshold=0.6, device='cpu'):\n",
    "    \"\"\"ê³ ê¸‰ í•„í„°ë§: Forgetì²˜ëŸ¼ ë³´ì´ë©´ì„œë„ í’ˆì§ˆì´ ì¢‹ì€ ì´ë¯¸ì§€ë§Œ ì„ ë³„\"\"\"\n",
    "    discriminator.eval()\n",
    "    device = torch.device(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images_gpu = images.to(device)\n",
    "        d_preds = discriminator(images_gpu).squeeze()\n",
    "        \n",
    "        # Forget íŠ¹ì„± ìœ ì§€ (ë‚®ì€ ê°’ = forgetìœ¼ë¡œ ë¶„ë¥˜)\n",
    "        forget_mask = d_preds < forget_threshold\n",
    "        \n",
    "        # í’ˆì§ˆ í•„í„°ë§\n",
    "        quality_mask = (d_preds > 0.1) & (d_preds < 0.9)\n",
    "        \n",
    "        # ìµœì¢… ë§ˆìŠ¤í¬\n",
    "        final_mask = forget_mask & quality_mask\n",
    "        \n",
    "        filtered_imgs = images[final_mask]\n",
    "        filtered_labels = labels[final_mask]\n",
    "        \n",
    "        print(f\"[Advanced Filter] {len(images)} â†’ {len(filtered_imgs)} images\")\n",
    "        print(f\"  Forget-like: {forget_mask.sum().item()}\")\n",
    "        print(f\"  Quality: {quality_mask.sum().item()}\")\n",
    "        print(f\"  Final: {final_mask.sum().item()}\")\n",
    "    \n",
    "    return filtered_imgs, filtered_labels\n",
    "\n",
    "\n",
    "def advanced_ungan_workflow(generator, discriminator, dataset, unseen_dataset,\n",
    "                           retain_idxs, forget_idxs, args, device):\n",
    "    \"\"\"Forget íŠ¹ì„± + Unseen ë³´ì•ˆì„±ì„ ê²°í•©í•œ ê³ ê¸‰ UNGAN ì›Œí¬í”Œë¡œìš°\"\"\"\n",
    "    print(\"\\n========== Advanced UNGAN: Forget + Unseen Fusion ==========\")\n",
    "    \n",
    "    # Step 1: Hybrid Generator í›ˆë ¨\n",
    "    print(\"Step 1: Training Hybrid Generator...\")\n",
    "    trained_generator = train_hybrid_generator(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator, \n",
    "        dataset=dataset,\n",
    "        unseen_dataset=unseen_dataset,\n",
    "        retain_idxs=retain_idxs,\n",
    "        forget_idxs=forget_idxs,\n",
    "        device=device,\n",
    "        alpha=0.7,  # Forget íŠ¹ì„± 70%\n",
    "        beta=0.3,   # Unseen íŠ¹ì„± 30%\n",
    "        z_dim=args.z_dim,\n",
    "        batch_size=args.local_bs,\n",
    "        epochs=30\n",
    "    )\n",
    "    \n",
    "    # Step 2: Hybrid ì´ë¯¸ì§€ ìƒì„±\n",
    "    print(\"Step 2: Generating Hybrid Images...\")\n",
    "    hybrid_images, hybrid_labels = generate_hybrid_images(\n",
    "        generator=trained_generator,\n",
    "        forget_idxs=forget_idxs,\n",
    "        unseen_dataset=unseen_dataset,\n",
    "        dataset=dataset,\n",
    "        device=device,\n",
    "        z_dim=args.z_dim,\n",
    "        style_strength=0.3\n",
    "    )\n",
    "    \n",
    "    # Step 3: ê³ ê¸‰ í•„í„°ë§\n",
    "    print(\"Step 3: Advanced Filtering...\")\n",
    "    filtered_images, filtered_labels = advanced_filter_images(\n",
    "        discriminator=discriminator,\n",
    "        images=hybrid_images,\n",
    "        labels=hybrid_labels,\n",
    "        forget_threshold=0.6,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated {len(hybrid_images)} hybrid samples\")\n",
    "    print(f\"Filtered to {len(filtered_images)} high-quality hybrid samples\")\n",
    "    if len(hybrid_images) > 0:\n",
    "        print(f\"Quality ratio: {len(filtered_images)/len(hybrid_images)*100:.1f}%\")\n",
    "    \n",
    "    return filtered_images, filtered_labels, trained_generator\n",
    "\n",
    "\n",
    "def evaluate_hybrid_quality(model, filtered_images, filtered_labels, \n",
    "                           forget_idxs, unseen_dataset, dataset, device):\n",
    "    \"\"\"Hybrid ë°ì´í„°ì˜ í’ˆì§ˆ í‰ê°€\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if len(filtered_images) == 0:\n",
    "        print(\"No filtered images to evaluate\")\n",
    "        return {}\n",
    "    \n",
    "    # 1. Forget íŠ¹ì„± ìœ ì§€ë„ í‰ê°€\n",
    "    forget_sample_size = min(50, len(forget_idxs))\n",
    "    forget_imgs = torch.stack([dataset[i][0] for i in forget_idxs[:forget_sample_size]])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Original forget ë°ì´í„° ì˜ˆì¸¡\n",
    "        forget_logits = model(forget_imgs.to(device))\n",
    "        forget_preds = F.softmax(forget_logits, dim=1)\n",
    "        \n",
    "        # Hybrid ë°ì´í„° ì˜ˆì¸¡\n",
    "        hybrid_sample_size = min(50, len(filtered_images))\n",
    "        hybrid_logits = model(filtered_images[:hybrid_sample_size].to(device))\n",
    "        hybrid_preds = F.softmax(hybrid_logits, dim=1)\n",
    "        \n",
    "        # í´ë˜ìŠ¤ ë¶„í¬ ìœ ì‚¬ì„±\n",
    "        class_similarity = F.cosine_similarity(\n",
    "            forget_preds.mean(dim=0), \n",
    "            hybrid_preds.mean(dim=0), \n",
    "            dim=0\n",
    "        ).item()\n",
    "    \n",
    "    # 2. ìŠ¤íƒ€ì¼ í†µê³„ ë¹„êµ\n",
    "    unseen_sample_size = min(50, len(unseen_dataset))\n",
    "    unseen_sample = torch.stack([unseen_dataset[i][0] for i in range(unseen_sample_size)])\n",
    "    \n",
    "    forget_stats = (forget_imgs.mean().item(), forget_imgs.std().item())\n",
    "    hybrid_stats = (filtered_images[:hybrid_sample_size].mean().item(), \n",
    "                   filtered_images[:hybrid_sample_size].std().item())\n",
    "    unseen_stats = (unseen_sample.mean().item(), unseen_sample.std().item())\n",
    "    \n",
    "    # Unseenì— ëŒ€í•œ ìœ ì‚¬ë„\n",
    "    style_similarity = 1 - abs(hybrid_stats[0] - unseen_stats[0]) - abs(hybrid_stats[1] - unseen_stats[1])\n",
    "    \n",
    "    results = {\n",
    "        'content_preservation': class_similarity,\n",
    "        'style_similarity': style_similarity,\n",
    "        'forget_stats': forget_stats,\n",
    "        'hybrid_stats': hybrid_stats,\n",
    "        'unseen_stats': unseen_stats\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n--- Hybrid Quality Evaluation ---\")\n",
    "    print(f\"Content Preservation: {class_similarity:.4f}\")\n",
    "    print(f\"Style Similarity: {style_similarity:.4f}\")\n",
    "    print(f\"Forget Stats: Î¼={forget_stats[0]:.3f}, Ïƒ={forget_stats[1]:.3f}\")\n",
    "    print(f\"Hybrid Stats: Î¼={hybrid_stats[0]:.3f}, Ïƒ={hybrid_stats[1]:.3f}\")\n",
    "    print(f\"Unseen Stats: Î¼={unseen_stats[0]:.3f}, Ïƒ={unseen_stats[1]:.3f}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4832c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== Main Functions ===================\n",
    "\n",
    "def move_dataset_to_device(dataset, device):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for x, y in dataset:\n",
    "        images.append(x.to(device))\n",
    "        labels.append(torch.tensor(y).to(device))\n",
    "    return TensorDataset(torch.stack(images), torch.stack(labels))\n",
    "\n",
    "\n",
    "def evaluate_backdoor_asr(model, dataset, target_label, device, dataset_type='cifar'):\n",
    "    \"\"\"ë°ì´í„°ì…‹ íƒ€ì…ì— ë”°ë¥¸ ë°±ë„ì–´ ASR í‰ê°€\"\"\"\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ íƒ€ì…ì— ë”°ë¥¸ ë°±ë„ì–´ í•¨ìˆ˜ ì„ íƒ\n",
    "    if dataset_type == 'cifar':\n",
    "        backdoor_func = add_backdoor_trigger_cifar\n",
    "    else:  # mnist\n",
    "        backdoor_func = add_backdoor_trigger_mnist\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            x, y = dataset[i]\n",
    "            x_bd = backdoor_func(x).to(device)\n",
    "            x_bd = x_bd.unsqueeze(0)\n",
    "\n",
    "            output = model(x_bd)\n",
    "            pred = output.argmax(dim=1).item()\n",
    "\n",
    "            total += 1\n",
    "            if pred == target_label:\n",
    "                correct += 1\n",
    "\n",
    "    asr = correct / total\n",
    "    return asr\n",
    "\n",
    "\n",
    "def select_model(args):\n",
    "    \"\"\"ë°ì´í„°ì…‹ê³¼ ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ\"\"\"\n",
    "    if args.dataset == 'cifar':\n",
    "        if args.model == 'cnn':\n",
    "            return CNNCifar(args=args)\n",
    "        elif args.model in ['resnet', 'resnet18']:\n",
    "            return ResNet18(num_classes=args.num_classes)\n",
    "    elif args.dataset == 'mnist':\n",
    "        if args.model == 'cnn':\n",
    "            return CNNMnist(args=args)\n",
    "        elif args.model in ['resnet', 'resnet18']:\n",
    "            # MNISTìš© ResNet18 (ì…ë ¥ ì±„ë„ 1ê°œë¡œ ìˆ˜ì •)\n",
    "            model = ResNet18(num_classes=args.num_classes)\n",
    "            # ì²« ë²ˆì§¸ conv ë ˆì´ì–´ë¥¼ 1ì±„ë„ë¡œ ë³€ê²½\n",
    "            model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            return model\n",
    "    \n",
    "    raise NotImplementedError(f\"Model {args.model} not implemented for {args.dataset}\")\n",
    "\n",
    "\n",
    "def select_generator_discriminator(args):\n",
    "    \"\"\"ë°ì´í„°ì…‹ì— ë§ëŠ” Generator/Discriminator ì„ íƒ\"\"\"\n",
    "    if args.dataset == 'cifar':\n",
    "        img_shape = (3, 32, 32)\n",
    "        generator = GeneratorCifar(z_dim=args.z_dim, img_shape=img_shape)\n",
    "        discriminator = DiscriminatorCifar(img_shape=img_shape)\n",
    "    elif args.dataset == 'mnist':\n",
    "        img_shape = (1, 28, 28)\n",
    "        generator = Generator(z_dim=args.z_dim, img_shape=img_shape)\n",
    "        discriminator = Discriminator(img_shape=img_shape)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Dataset {args.dataset} not supported\")\n",
    "    \n",
    "    return generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f29af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    args = args_parser()\n",
    "    device = 'cuda' if args.gpu and torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    exp_details(args)\n",
    "\n",
    "    # ===================== 1. ë°ì´í„°ì…‹ ë¡œë”© ë° ì´ˆê¸°í™” =====================\n",
    "    train_dataset, test_dataset, unseen_dataset, user_groups = get_dataset(args)\n",
    "    \n",
    "    print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "    print(f\"Test dataset: {len(test_dataset)} samples\") \n",
    "    print(f\"Unseen dataset: {len(unseen_dataset)} samples\")\n",
    "    \n",
    "    # ë°±ë„ì–´ ë…ì„± ë°ì´í„°ì…‹ ìƒì„±\n",
    "    full_dataset, user_groups = create_poisoned_dataset(train_dataset, user_groups, args,\n",
    "                                                        malicious_client=0,\n",
    "                                                        target_label=6,\n",
    "                                                        poison_ratio=0.8)\n",
    "\n",
    "    # ëª¨ë¸ ì´ˆê¸°í™” (ë°ì´í„°ì…‹ë³„ ìë™ ì„ íƒ)\n",
    "    global_model = select_model(args).to(device)\n",
    "    global_model.train()\n",
    "\n",
    "    # Generator/Discriminator ì´ˆê¸°í™” (ë°ì´í„°ì…‹ë³„ ìë™ ì„ íƒ)\n",
    "    generator, discriminator = select_generator_discriminator(args)\n",
    "    generator = generator.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "\n",
    "    global_weights = global_model.state_dict()\n",
    "    train_loss, train_accuracy = [], []\n",
    "\n",
    "    # Unlearning ëŒ€ìƒ ì„¤ì •\n",
    "    forget_client = 0\n",
    "    forget_idxs = user_groups[forget_client]\n",
    "    retain_idxs = [i for i in range(len(train_dataset)) if i not in forget_idxs]\n",
    "    test_idxs = np.random.choice(len(test_dataset), min(len(forget_idxs), len(test_dataset)), replace=False)\n",
    "\n",
    "    print(f\"\\nModel: {args.model.upper()}\")\n",
    "    print(f\"Forget client: {forget_client}\")\n",
    "    print(f\"Forget data size: {len(forget_idxs)}\")\n",
    "    print(f\"Retain data size: {len(retain_idxs)}\")\n",
    "\n",
    "    # ===================== 2. ì—°í•© í•™ìŠµ (FedAvg) =====================\n",
    "    print(\"\\n========== Starting Federated Learning ==========\")\n",
    "    for epoch in tqdm(range(args.epochs), desc='Global Training Rounds'):\n",
    "        local_weights, local_losses = [], []\n",
    "        m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "        for idx in idxs_users:\n",
    "            if idx == forget_client:\n",
    "                continue  # ì•…ì„± í´ë¼ì´ì–¸íŠ¸ ì œì™¸\n",
    "\n",
    "            local_model = LocalUpdate(args=args, dataset=full_dataset, idxs=user_groups[idx])\n",
    "            w, loss = local_model.update_weights(model=copy.deepcopy(global_model), global_round=epoch)\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(loss)\n",
    "\n",
    "        if local_weights:\n",
    "            global_weights = average_weights(local_weights)\n",
    "            global_model.load_state_dict(global_weights)\n",
    "\n",
    "        loss_avg = sum(local_losses) / len(local_losses) if local_losses else 0\n",
    "        acc, _ = test_inference(args, global_model, test_dataset)\n",
    "        train_loss.append(loss_avg)\n",
    "        train_accuracy.append(acc)\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Round {epoch+1}: Loss {loss_avg:.4f}, Acc {acc*100:.2f}%\")\n",
    "    \n",
    "    federated_time = time.time() - start_time\n",
    "    print(f\"\\n========== Federated Learning Completed ==========\")\n",
    "    print(f\"Training Time: {federated_time:.2f}s\")\n",
    "\n",
    "    # ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ë°±ì—…\n",
    "    pretrained_model = copy.deepcopy(global_model)\n",
    "\n",
    "    # ===================== 3. Advanced UNGAN Unlearning =====================\n",
    "    unlearn_start_time = time.time()\n",
    "    \n",
    "    # Advanced UNGAN ì›Œí¬í”Œë¡œìš° ì‹¤í–‰\n",
    "    filtered_images, filtered_labels, trained_generator = advanced_ungan_workflow(\n",
    "        generator, discriminator, full_dataset, unseen_dataset,\n",
    "        retain_idxs, forget_idxs, args, device\n",
    "    )\n",
    "\n",
    "    # ğŸ¨ ì‹œê°í™” ì¶”ê°€!\n",
    "    visualize_advanced_ungan_results(\n",
    "        trained_generator, discriminator, full_dataset, unseen_dataset,\n",
    "        forget_idxs, filtered_images, filtered_labels, device, args, dataset_type=args.dataset\n",
    "    )\n",
    "\n",
    "    # Hybrid í’ˆì§ˆ í‰ê°€\n",
    "    if len(filtered_images) > 0:\n",
    "        quality_results = evaluate_hybrid_quality(\n",
    "            global_model, filtered_images, filtered_labels,\n",
    "            forget_idxs, unseen_dataset, full_dataset, device\n",
    "        )\n",
    "    else:\n",
    "        quality_results = {}\n",
    "        print(\"Warning: No high-quality synthetic data generated\")\n",
    "\n",
    "    # Synthetic ë°ì´í„°ë¡œ Unlearning ìˆ˜í–‰\n",
    "    if len(filtered_images) > 0:\n",
    "        print(\"\\n========== Performing Unlearning with Synthetic Data ==========\")\n",
    "        \n",
    "        # Synthetic Dataset ìƒì„±\n",
    "        synthetic_dataset = SyntheticImageDataset(filtered_images, filtered_labels)\n",
    "        \n",
    "        # Unlearning í›ˆë ¨\n",
    "        unlearned_model = copy.deepcopy(pretrained_model)\n",
    "        unlearned_model.train()\n",
    "        \n",
    "        for unlearn_epoch in range(10):  # Unlearning ì—í¬í¬\n",
    "            local_weights, local_losses = [], []\n",
    "            \n",
    "            # Retain í´ë¼ì´ì–¸íŠ¸ë“¤ë§Œ ì°¸ì—¬\n",
    "            for idx in range(1, args.num_users):\n",
    "                local_model = LocalUpdate(args=args, dataset=full_dataset, idxs=user_groups[idx])\n",
    "                w, loss = local_model.update_weights(\n",
    "                    model=copy.deepcopy(unlearned_model), \n",
    "                    global_round=unlearn_epoch\n",
    "                )\n",
    "                local_weights.append(copy.deepcopy(w))\n",
    "                local_losses.append(loss)\n",
    "            \n",
    "            # Synthetic ë°ì´í„°ë¡œ í›ˆë ¨\n",
    "            synthetic_local = LocalUpdate(args=args, dataset=synthetic_dataset, idxs=None)\n",
    "            w_syn, loss_syn = synthetic_local.update_weights(\n",
    "                model=copy.deepcopy(unlearned_model),\n",
    "                global_round=unlearn_epoch\n",
    "            )\n",
    "            local_weights.append(copy.deepcopy(w_syn))\n",
    "            local_losses.append(loss_syn)\n",
    "            \n",
    "            # ê¸€ë¡œë²Œ ëª¨ë¸ ì—…ë°ì´íŠ¸\n",
    "            if local_weights:\n",
    "                unlearned_weights = average_weights(local_weights)\n",
    "                unlearned_model.load_state_dict(unlearned_weights)\n",
    "            \n",
    "            if (unlearn_epoch + 1) % 5 == 0:\n",
    "                loss_avg = sum(local_losses) / len(local_losses)\n",
    "                acc, _ = test_inference(args, unlearned_model, test_dataset)\n",
    "                print(f\"Unlearn Epoch {unlearn_epoch + 1}: Loss {loss_avg:.4f}, Acc {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(\"Skipping unlearning due to insufficient synthetic data\")\n",
    "        unlearned_model = pretrained_model\n",
    "\n",
    "    unlearn_time = time.time() - unlearn_start_time\n",
    "\n",
    "    # ===================== 4. ì¢…í•© í‰ê°€ =====================\n",
    "    print(\"\\n========== Comprehensive Evaluation ==========\")\n",
    "    \n",
    "    # ì„±ëŠ¥ ë¹„êµ\n",
    "    test_acc_before, test_loss_before = test_inference(args, pretrained_model, test_dataset)\n",
    "    test_acc_after, test_loss_after = test_inference(args, unlearned_model, test_dataset)\n",
    "    \n",
    "    print(f\"[Test Performance]\")\n",
    "    print(f\"  Before: {test_acc_before*100:.2f}% | After: {test_acc_after*100:.2f}%\")\n",
    "    print(f\"  Retention: {(test_acc_after/test_acc_before)*100:.1f}%\")\n",
    "\n",
    "    # MIA í‰ê°€\n",
    "    print(f\"\\n[MIA Evaluation]\")\n",
    "    all_idxs = set(range(len(full_dataset)))\n",
    "    non_member_candidates = list(all_idxs - set(forget_idxs))\n",
    "    shadow_idxs = np.random.choice(non_member_candidates, \n",
    "                                 min(len(forget_idxs), len(non_member_candidates)), \n",
    "                                 replace=False)\n",
    "    \n",
    "    mia_before = evaluate_mia(\n",
    "        model=pretrained_model, dataset=full_dataset, test_dataset=test_dataset,\n",
    "        forget_idxs=forget_idxs, retain_idxs=test_idxs, shadow_idxs=shadow_idxs,\n",
    "        device=device, save_path=\"./mia_before_advanced.json\"\n",
    "    )\n",
    "    \n",
    "    mia_after = evaluate_mia(\n",
    "        model=unlearned_model, dataset=full_dataset, test_dataset=test_dataset,\n",
    "        forget_idxs=forget_idxs, retain_idxs=test_idxs, shadow_idxs=shadow_idxs,\n",
    "        device=device, save_path=\"./mia_after_advanced.json\"\n",
    "    )\n",
    "    \n",
    "    print(f\"  Before: {mia_before['auc']:.4f} | After: {mia_after['auc']:.4f}\")\n",
    "    print(f\"  Privacy Gain: {mia_before['auc'] - mia_after['auc']:.4f}\")\n",
    "\n",
    "    # ë°±ë„ì–´ ASR í‰ê°€\n",
    "    print(f\"\\n[Backdoor ASR Evaluation]\")\n",
    "    target_label = 6\n",
    "    asr_before = evaluate_backdoor_asr(pretrained_model, test_dataset, target_label, device, args.dataset)\n",
    "    asr_after = evaluate_backdoor_asr(unlearned_model, test_dataset, target_label, device, args.dataset)\n",
    "    \n",
    "    print(f\"  Before: {asr_before*100:.2f}% | After: {asr_after*100:.2f}%\")\n",
    "    print(f\"  Attack Reduction: {(asr_before - asr_after)*100:.2f}%\")\n",
    "\n",
    "    # Unseen Dataset í‰ê°€\n",
    "    if unseen_dataset is not None:\n",
    "        print(f\"\\n[Unseen Dataset Evaluation]\")\n",
    "        unseen_acc_before, _ = test_inference(args, pretrained_model, unseen_dataset)\n",
    "        unseen_acc_after, _ = test_inference(args, unlearned_model, unseen_dataset)\n",
    "        \n",
    "        print(f\"  Before: {unseen_acc_before*100:.2f}% | After: {unseen_acc_after*100:.2f}%\")\n",
    "        print(f\"  Preservation: {(unseen_acc_after/unseen_acc_before)*100:.1f}%\")\n",
    "\n",
    "    # ===================== 5. ê²°ê³¼ ì €ì¥ ë° ìš”ì•½ =====================\n",
    "    \n",
    "    # ëª¨ë¸ ì €ì¥\n",
    "    torch.save(pretrained_model.state_dict(), args.save_model)\n",
    "    unlearned_save_path = args.save_model.replace('.pth', '_unlearned.pth')\n",
    "    torch.save(unlearned_model.state_dict(), unlearned_save_path)\n",
    "    \n",
    "    # ì¢…í•© ê²°ê³¼\n",
    "    total_time = time.time() - start_time\n",
    "    results = {\n",
    "        'experiment_type': 'Advanced_UNGAN_Federated_Unlearning',\n",
    "        'configuration': {\n",
    "            'dataset': args.dataset,\n",
    "            'model': args.model,\n",
    "            'num_users': args.num_users,\n",
    "            'epochs': args.epochs,\n",
    "            'forget_client': forget_client,\n",
    "        },\n",
    "        'timing': {\n",
    "            'federated_learning': federated_time,\n",
    "            'unlearning': unlearn_time,\n",
    "            'total': total_time\n",
    "        },\n",
    "        'performance': {\n",
    "            'test_acc_before': float(test_acc_before),\n",
    "            'test_acc_after': float(test_acc_after),\n",
    "            'retention_rate': float(test_acc_after / test_acc_before)\n",
    "        },\n",
    "        'privacy': {\n",
    "            'mia_auc_before': float(mia_before['auc']),\n",
    "            'mia_auc_after': float(mia_after['auc']),\n",
    "            'privacy_improvement': float(mia_before['auc'] - mia_after['auc'])\n",
    "        },\n",
    "        'security': {\n",
    "            'asr_before': float(asr_before),\n",
    "            'asr_after': float(asr_after),\n",
    "            'attack_mitigation': float(asr_before - asr_after)\n",
    "        },\n",
    "        'synthetic_data': {\n",
    "            'generated_samples': len(filtered_images) if len(filtered_images) > 0 else 0,\n",
    "            'quality_metrics': quality_results\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # ê²°ê³¼ JSON ì €ì¥\n",
    "    with open('./advanced_ungan_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e07b2b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # ResNet18 + CIFAR-10ìš© ì‹¤í–‰ íŒŒë¼ë¯¸í„°\n",
    "    import sys\n",
    "    sys.argv = [\n",
    "        'main.py',\n",
    "        '--epochs', '100',         # ì‹¤í—˜ìš©ìœ¼ë¡œ ì¤„ì„\n",
    "        '--num_users', '10',\n",
    "        '--frac', '1.0',\n",
    "        '--local_ep', '10',        # ì‹¤í—˜ìš©ìœ¼ë¡œ ì¤„ì„  \n",
    "        '--local_bs', '64',\n",
    "        '--lr', '0.01',\n",
    "        '--momentum', '0.9',\n",
    "        '--dataset', 'mnist',\n",
    "        '--model', 'cnn',    # ResNet18 ì‚¬ìš©\n",
    "        '--iid', '1',\n",
    "        '--gpu', '0',\n",
    "        '--num_classes', '10',\n",
    "        '--load_model', 'None',\n",
    "        '--save_model', './saved_models/advanced_cifar_resnet18.pth',\n",
    "        '--z_dim', '100',\n",
    "        '--gen_threshold', '0.7',  # í•„í„°ë§ ì„ê³„ê°’ ì¡°ì •\n",
    "        '--num_gen_samples', '128'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72d208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Experiment Settings =====\n",
      "Model           : cnn\n",
      "Dataset         : mnist\n",
      "Num Clients     : 10\n",
      "Fraction        : 1.0\n",
      "IID             : 1\n",
      "Local Epochs    : 10\n",
      "Batch Size      : 64\n",
      "Learning Rate   : 0.01\n",
      "Generator z_dim : 100\n",
      "Disc. Threshold : 0.7\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:02<00:00, 4.91MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 145kB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:01<00:00, 1.48MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 1.57MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 55000 samples\n",
      "Test dataset: 10000 samples\n",
      "Unseen dataset: 5000 samples\n",
      "\n",
      "Model: CNN\n",
      "Forget client: 0\n",
      "Forget data size: 5500\n",
      "Retain data size: 49500\n",
      "\n",
      "========== Starting Federated Learning ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global Training Rounds:   5%|â–Œ         | 5/100 [01:24<26:25, 16.69s/it]"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
